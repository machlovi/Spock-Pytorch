{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/machlovi/Spock-paper/blob/main/X_hate_and_offensive_data_combine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdBpcROn7EF-"
   },
   "source": [
    "## This file uses HateXplain data\n",
    "We are initailizing 2 vector space for 3(after converting them into 2 class) classes in this code. Main idea is to visualize how we can separate words related to each classes in the given space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4UYfxZaZz6j8",
    "outputId": "09a73fe3-70ee-4cca-c198-fafbeda639f7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! apt-get install git\n",
    "# !pip install --upgrade protobuf\n",
    "# !pip install --upgrade jupyterlab-server google-api-core cached-path alchemy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj4pk2f97ALa"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7fS4-4j6y_h",
    "outputId": "6b5de3d8-1951-4e39-8a5e-2a5759d5f38d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/naseem_fordham/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/naseem_fordham/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm, trange\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_rows\",20)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# from datasets import load_dataset,Dataset\n",
    "from transformers import AutoModel, BertTokenizerFast, BertModel\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Importing loss file\n",
    "from custom_loss import CustomLoss, CosineSimilarityLoss, IntraClassLoss, BinaryCrossEntropyLoss, LossValues\n",
    "from BERTdata_loader import BERTDataset\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "# Ignore UserWarning and UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from preprocess import TextPreprocessor  # Import your TextPreprocessor class\n",
    "text_processor = TextPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To display full column and rows values\n",
    "# pd.set_option('display.max_column', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_seq_items', None)\n",
    "# pd.set_option('display.max_colwidth', 500)\n",
    "# pd.set_option('expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install numba\n",
    "\n",
    "def on_gpu(f):\n",
    "    def wrapper(*args):\n",
    "        if torch.cuda.is_available():\n",
    "            return f(*args)\n",
    "        else:\n",
    "            print('cuda unavailable')\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This part of the code uses cuda for GPU utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # ! pip install pynvml\n",
    "    from pynvml import *\n",
    "    from numba import cuda\n",
    "\n",
    "@on_gpu\n",
    "def print_gpu_utilization(dev_id):\n",
    "    try:\n",
    "        nvmlInit()\n",
    "        handle = nvmlDeviceGetHandleByIndex(dev_id)\n",
    "        info = nvmlDeviceGetMemoryInfo(handle)\n",
    "        print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "@on_gpu\n",
    "def free_gpu_cache(dev_id=0):\n",
    "    print(\"Initial GPU Usage\")\n",
    "    print_gpu_utilization(dev_id)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    print_gpu_utilization(dev_id)\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device_id = 0\n",
    "# device = torch.device(f'cuda:{device_id}' if torch.cuda.is_available() else 'cpu')\n",
    "# torch.cuda.set_device(device)\n",
    "# print(device)\n",
    "\n",
    "# print_gpu_utilization(device_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #loading hate ofefsive data and converting it into 2 class data as toxic and non-toxic\n",
    "# df_data1=pd.read_csv('/home/naseem_fordham/Pytorch/data/IMDB_Dataset.csv')\n",
    "# # df_data1.rename(columns={'review':'tweet', 'sentiment':'class'})\n",
    "# df_data1 = df_data1.rename(columns={'review':'tweet', 'sentiment':'class'})\n",
    "\n",
    "# dataframe=df_data1[['class','tweet']]\n",
    "# dataframe=dataframe.dropna()\n",
    "# dataframe.reset_index(drop=True)\n",
    "# dataframe['class'].unique()\n",
    "\n",
    "# # #initially we have 3 classes and now we are converting them into binary class\n",
    "\n",
    "# dataframe[\"class\"] = dataframe[\"class\"].apply(lambda x: 1.0 if x =='negative' else 0.0)\n",
    "# # HateXplain[\"label\"] = HateXplain[\"label\"].apply(lambda x: 1.0 if x in [\"hatespeech\", \"offensive\"] else 0.0)\n",
    "\n",
    "# dataframe[\"tweet\"] = dataframe[\"tweet\"].apply(lambda x : text_processor.text_preprocessing(x))\n",
    "# dataframe = dataframe.sample(frac = 1).reset_index(drop = True)\n",
    "\n",
    "# # dataframe=HateXplain\n",
    "# dataframe['class'].unique()\n",
    "# # dataframe.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HateOffensive data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lp7FNeAE7M-s",
    "tags": []
   },
   "source": [
    "https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data#Data import\n",
    "\n",
    "Data Source \"https://github.com/t-davidson/33 ##\n",
    "hate-speech-and-offensive-language/blob/master/data/readme.md\"\n",
    "'''hate_speech = number of CF users who judged the tweet to be hate speech.\n",
    "offensive_language = number of CF users who judged the tweet to be offensive.\n",
    "neither = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n",
    "class = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither'''\n",
    "In this case 0 non-toxic and 1 -toxic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading hate ofefsive data and converting it into 2 class data as toxic and non-toxic\n",
    "df_data1=pd.read_csv(\"/home/naseem_fordham/Spock-paper/data/labeled_data.csv\")\n",
    "dataframe=df_data1[['class','tweet']]\n",
    "dataframe=dataframe.dropna()\n",
    "dataframe.reset_index(drop=True)\n",
    "dataframe['class'].unique()\n",
    "\n",
    "# #initially we have 3 classes and now we are converting them into binary class\n",
    "\n",
    "dataframe[\"class\"] = dataframe[\"class\"].apply(lambda x: 1.0 if x in [0., 1.] else 0.0)\n",
    "# HateXplain[\"label\"] = HateXplain[\"label\"].apply(lambda x: 1.0 if x in [\"hatespeech\", \"offensive\"] else 0.0)\n",
    "\n",
    "dataframe[\"tweet\"] = dataframe[\"tweet\"].apply(lambda x : text_processor.text_preprocessing(x))\n",
    "dataframe = dataframe.sample(frac = 1).reset_index(drop = True)\n",
    "\n",
    "# dataframe=HateXplain\n",
    "dataframe['class'].unique()\n",
    "# dataframe.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>wellis he gonna take da bitch or naw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>look bitch go and suck a dick im a real street...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>nigga relax i cheat on my girl with regular bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>if a bitch dont taste her own you shouldnt eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>prophzilla before i went off to college i want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38890</th>\n",
       "      <td>1.0</td>\n",
       "      <td>rt preemasterflex females be acting turning up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38891</th>\n",
       "      <td>1.0</td>\n",
       "      <td>y though really hoe u the dummy taking a pic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38892</th>\n",
       "      <td>1.0</td>\n",
       "      <td>ill round up my hoes in a little bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38893</th>\n",
       "      <td>1.0</td>\n",
       "      <td>rt fuck michelle obama that monkey doesnt belo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38894</th>\n",
       "      <td>1.0</td>\n",
       "      <td>everybody got foreign bitches and foreign cars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38895 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                                              tweet\n",
       "0        1.0               wellis he gonna take da bitch or naw\n",
       "1        1.0  look bitch go and suck a dick im a real street...\n",
       "2        1.0  nigga relax i cheat on my girl with regular bi...\n",
       "3        1.0  if a bitch dont taste her own you shouldnt eve...\n",
       "4        1.0  prophzilla before i went off to college i want...\n",
       "...      ...                                                ...\n",
       "38890    1.0  rt preemasterflex females be acting turning up...\n",
       "38891    1.0       y though really hoe u the dummy taking a pic\n",
       "38892    1.0               ill round up my hoes in a little bit\n",
       "38893    1.0  rt fuck michelle obama that monkey doesnt belo...\n",
       "38894    1.0     everybody got foreign bitches and foreign cars\n",
       "\n",
       "[38895 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Assuming 'dataframe' is your DataFrame\n",
    "# class_1_data = dataframe[dataframe['class'] == 1.0].sample(n=100, random_state=1)\n",
    "# class_0_data = dataframe[dataframe['class'] == 0.0].sample(n=100, random_state=1)\n",
    "\n",
    "# # Concatenate both class samples\n",
    "# selected_data = pd.concat([class_0_data, class_1_data])\n",
    "\n",
    "# # Shuffle the selected data\n",
    "# selected_data = selected_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "# dataframe=selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "-Y5XnqHVWi9T",
    "outputId": "868f2ad5-0f5c-41c8-b705-f1b3ad8f012f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# token lengths distribution in the dataset\n",
    "token_lengths = [len(i.split()) for i in dataframe[\"tweet\"]]\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.hist(token_lengths,bins = 30,edgecolor=\"black\")\n",
    "plt.xticks(ticks = np.linspace(10,100,10))\n",
    "plt.show()\n",
    "dataframe[\"token_length\"] = token_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQEmSl2aS7_T"
   },
   "source": [
    "# Training and validation set\n",
    "We can also consider token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "6M-IQ2ZyYVwb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# token_length 128, seems a good fit for data\n",
    "\n",
    "# split training and validation data\n",
    "train_df, val_df = train_test_split(dataframe, test_size= 0.20, stratify= dataframe[\"class\"], random_state = 40)\n",
    "\n",
    "# val_df,   test_df   = train_test_split(temp_df, test_size= 0.80, stratify= temp_df[\"class\"],random_state = 47)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df   = val_df.reset_index(drop  = True)\n",
    "# test_df  = test_df.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "# bertModel = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\").to(device)\n",
    "# bertModel.trainable = False # freezing the weights\n",
    "\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "\n",
    "# encoded_input = tokenizer(text, return_tensors='pt').to(device)\n",
    "# model.to(device)\n",
    "\n",
    "# output = model(**encoded_input)\n",
    "\n",
    "#getting model total number of parameters\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# count_parameters(bertModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286,
     "referenced_widgets": [
      "570b9a872e614e0eab6618f306be7306",
      "34e574772e8b4f81bc7e02e8ebaeb258",
      "1cf66f2a8c5f49ce98d59dae5186298d",
      "71f100b04c964a1bbde06c537f483ed7",
      "9e1caf8b23fb4eb1976f808269d1dd3e",
      "3258d106659040dfb7ffea47c017ed38",
      "f9a2caba063b4a4f98ad258044d012fb",
      "dfd44792f624499b87bd768836016e88",
      "ae841edad05b4ef9b4308308d6683fa3",
      "34bbd1edf06c4a9194bd27eaeeee32b2",
      "ea850e74cb6a4121a456578b29ce7955",
      "d69dfd21bce04204b3220b71b4d4a968",
      "4914d10c0f354e178de66dc11449e26a",
      "f554ec9fe80a408680aa7b0a1c6837ac",
      "f4ec333b7c654876a6caad010e88c203",
      "cfe933364a3147368660f5463f1e5a29",
      "f7998e50af804d66b6af0f26b5588255",
      "4fa52eecbe8a4ba38ef969fd1cf48cfd",
      "3c294ab0d9a648f39ea3f3fb7dd08c3c",
      "31914a14cfa243388f43a02da4db24b6",
      "69d90bdca5174dfb91f5cf124884095e",
      "123641d1426541fba838576dff7f6082",
      "71d0c2b02f0c4a609477bc31aa880938",
      "c12c857f159443f1b2a661fcd7afa002",
      "989933f9c84a47d4b64d5b5441a1f492",
      "2024092105904b418c984aae9f4118bf",
      "4cc5655f9bdb42bca5d80e2849e8cd2b",
      "fd56d3ae17774e5496a7fdcbcb57b874",
      "b5516712d5304670a05ec871eb5896a4",
      "9910be9c8b7b4188936721e958ca15c5",
      "50859aa086814457bcab249b35d486a8",
      "4f0a4e1b4929448a8b0a25b202271a40",
      "ac93c61a19c0474994803853194d2aa8",
      "ee12049f5a9c4505b9769a4ff9c36477",
      "18e134c7af5d45baa952ccf06f96f3d8",
      "21bf2e03eac8419fa693628ab2cef02d",
      "0f14c359d92748268810099e6cdd1fc3",
      "e99b3d05437f4cdf9e1700e3cf466b34",
      "4f1d5a79d7ec4e42add7775fa925b187",
      "e89d668a4fb541ec8d75d171bf899869",
      "430a9d1768614e0596960d2a3d15d69e",
      "909ae03e117345f7a4b53f3045e090b7",
      "06285d669e92494192637cb3ee5a40f4",
      "209ee3e0949b446b8c58c57989065c5a",
      "672e978f9b524c5784553abd8cc91507",
      "17ae300ebf634778862bc211cb432eef",
      "5a072e1f73624ea5aa35e7e18af17f50",
      "1ec35f8bfadb40968f777ab7bbb55184",
      "274875a4aa6047c9903ae3065a5d85c2",
      "a51f1ae4e3bd4563be3eb7c04dec7a60",
      "c7586ac9b41c4e75b02a6c076a458686",
      "8a0c04349bae44f4a7130463ded826d1",
      "9f11959bab964aad822250b6e6853cb8",
      "9b0c918326174bc4b80585a52ac33e32",
      "e38ea829111b4065bad8ad4a4927bc39"
     ]
    },
    "id": "qe9vOPP6TD5-",
    "outputId": "d30dde8d-148a-4d7c-d57d-05ebeb164a73",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # # load bertModel, bertTokenizer and freeze all layers\n",
    "# bertModel = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "\n",
    "# bertModel.trainable = False # freezing the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #getting model total number of parameters\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# count_parameters(bertModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation for BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from BERTdata_loader_noattention import BERTDataset\n",
    "bert_dataset = BERTDataset(text=None, labels=None,  max_length=None, tokenizer=None, projection_dim=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGGXmXltToJP"
   },
   "source": [
    "# Paramters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xVIgFoIztcq"
   },
   "source": [
    "Loading Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class SpockModel(nn.Module):\n",
    "    def __init__(self,bertModel, MAX_LENGTH, PROJECTION_DIM, lambda_value):\n",
    "        super(SpockModel, self).__init__()\n",
    "        self.MAX_LENGTH = MAX_LENGTH\n",
    "        self.PROJECTION_DIM = PROJECTION_DIM\n",
    "        self.lambda_value = lambda_value\n",
    "        self.VECTOR_DIM = 768\n",
    "\n",
    "        # Layers\n",
    "        self.bert_model = bertModel  # Assuming you have defined bertModel elsewhere\n",
    "        self.offensive_embedding_layer = nn.Embedding(PROJECTION_DIM, self.VECTOR_DIM) # (None,PROJECTION_DIM,VECTOR_DIM)\n",
    "        self.normal_embedding_layer = nn.Embedding(PROJECTION_DIM, self.VECTOR_DIM)   # (None,PROJECTION_DIM,VECTOR_DIM)\n",
    "\n",
    "        self.hidden1 = nn.Linear(2 * self.PROJECTION_DIM, 612)\n",
    "        self.dropout1=nn.Dropout(0.5)\n",
    "        \n",
    "        self.hidden2 = nn.Linear(612, 256)\n",
    "        self.dropout2=nn.Dropout(0.5)\n",
    "        \n",
    "        self.hidden3 = nn.Linear(256, 64)\n",
    "        self.dropout3=nn.Dropout(0.5)\n",
    "        \n",
    "#         self.hidden4 = nn.Linear(64, 30)\n",
    "#         self.dropout4=nn.Dropout(0.5)\n",
    "        \n",
    "        # self.hidden5 = nn.Linear(612, 10)\n",
    "        self.classification_layer = nn.Linear(612, 1)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Loss Functions\n",
    "        self.cosine_loss = CosineSimilarityLoss(name='cosine_loss')\n",
    "        self.intra_loss = IntraClassLoss(name='intra_loss')\n",
    "        self.bce_loss = BinaryCrossEntropyLoss(name='attention_loss')\n",
    "        # self.loss_values = LossValues()\n",
    "\n",
    "\n",
    "    def forward(self, ids, mks, projection_space):\n",
    "        input_sentence = self.bert_model(ids, attention_mask=mks)[0]   #(None, Max_Len,VECTOR_DIM)\n",
    "        # print(input_sentence.shape)\n",
    "        offensive_embedding_np = self.offensive_embedding_layer(projection_space)  # (None,PROJECTION_DIM,VECTOR_DIM)\n",
    "        normal_embedding_np = self.normal_embedding_layer(projection_space)        # (None,PROJECTION_DIM,VECTOR_DIM)\n",
    "        # print(normal_embedding_np.shape)\n",
    "        offensive_embedding = offensive_embedding_np.permute(0, 2, 1)              #(None,PROJECTION_DIM, VECTOR_DIM) --> (None,VECTOR_DIM,PROJECTION_DIM)\n",
    "        normal_embedding = normal_embedding_np.permute(0, 2, 1)\n",
    "        # print(normal_embedding.shape)\n",
    "\n",
    "        offensive_cosine = self.cosine_similarity_projected(input_sentence, offensive_embedding) #  (None, Max_Len,VECTOR_DIM). (None,VECTOR_DIM,PROJECTION_DIM) -->((None, Max_Len, Projection)\n",
    "        normal_cosine = self.cosine_similarity_projected(input_sentence, normal_embedding)\n",
    "        # print(normal_cosine.shape)\n",
    "\n",
    "        \n",
    "        offensive_cosine_nopads = self.remove_padsV2(offensive_cosine, ids)\n",
    "        normal_cosine_nopads = self.remove_padsV2(normal_cosine, ids)\n",
    "\n",
    "        \n",
    "        \n",
    "        merged = self.merge_functionV2(offensive_cosine_nopads, normal_cosine_nopads)\n",
    "        merged = merged.view(-1, 2 * self.PROJECTION_DIM)\n",
    "\n",
    "        hidden1 = torch.tanh(self.hidden1(merged))\n",
    "        hidden1_output = self.dropout1(hidden1)\n",
    "        \n",
    "#         hidden2 = torch.tanh(self.hidden2(hidden1_output))\n",
    "#         hidden2_output=self.dropout1(hidden2)\n",
    "        \n",
    "#         hidden3 = torch.tanh(self.hidden3(hidden2_output))\n",
    "#         hidden3_output=self.dropout1(hidden3)\n",
    "        \n",
    "#         hidden4 = torch.tanh(self.hidden4(hidden3_output))\n",
    "#         hidden4_output=self.dropout1(hidden4)\n",
    "\n",
    "        \n",
    "#         hidden5 = torch.tanh(self.hidden5(hidden1_output))\n",
    "#         hidden5_output=self.dropout1(hidden5)\n",
    "\n",
    "\n",
    "        predictions = self.classification_layer(hidden1_output)\n",
    "        \n",
    "\n",
    "        # # Losses\n",
    "        offensive_normal_loss = self.cosine_loss(torch.mean(offensive_cosine, dim=1), torch.mean(normal_cosine, dim=1))\n",
    "        toxic_intra_loss = self.intra_loss(offensive_cosine)\n",
    "        non_toxic_intra_loss = self.intra_loss(normal_cosine)\n",
    "\n",
    "        # Total Loss\n",
    "        # + self.lambda_value * bce_loss1\n",
    "        loss = offensive_normal_loss + toxic_intra_loss + non_toxic_intra_loss \n",
    "        \n",
    "        \n",
    "        return predictions,loss,offensive_normal_loss, toxic_intra_loss, non_toxic_intra_loss\n",
    "    \n",
    "    \n",
    "    def cosine_similarity_projected(self, x, w):\n",
    "        dp = torch.matmul(x, w)\n",
    "        x_mag = torch.norm(x, dim=2, keepdim=True)\n",
    "        w_mag = torch.norm(w, dim=1, keepdim=True)\n",
    "        cosine = dp / (x_mag * w_mag)\n",
    "        return cosine\n",
    "    \n",
    "\n",
    "\n",
    "    def merge_functionV2(self, negative, normal):\n",
    "        negative_max = torch.max(negative, dim=1, keepdim=True)[0]\n",
    "        normal_max = torch.max(normal, dim=1, keepdim=True)[0]\n",
    "        return torch.cat([negative_max, normal_max], dim=-1)\n",
    "    \n",
    "    def remove_padsV2(self, vects, ids):\n",
    "        # masks = ids != 0\n",
    "        masks = (ids != 101) & (ids != 102)\n",
    "        masks = masks.unsqueeze(-1).float()\n",
    "        masked_embeddings = vects * masks\n",
    "\n",
    "        # print(masked_embeddings.shape)\n",
    "        return masked_embeddings\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Training and Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31116"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred)[0].sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    # print(torch.eq(y_true, y_pred)[0])\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Simple training but we can reduce the training time by using Ray training. For better understanding follow the documentation'''\n",
    "\n",
    "# MAX_LEN = 30\n",
    "# BATCH_SIZE=32\n",
    "# lambda_value=40\n",
    "# learning_rate=1e-3\n",
    "# num_epochs=5\n",
    "# PROJECTION_DIM=10\n",
    "\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# train_dataloader,val_dataloader = bert_dataset.dataprep(train_df, val_df, tokenizer, MAX_LEN, BATCH_SIZE,PROJECTION_DIM)\n",
    "\n",
    "\n",
    "# # Define loss function and optimizer\n",
    "# # criterion = torch.nn.BCELoss()\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# space_model = SpockModel(MAX_LEN, PROJECTION_DIM, lambda_value).to(device)\n",
    "\n",
    "# # optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# optimizer = optim.SGD(space_model.parameters(), lr=learning_rate,momentum=0.4)\n",
    "\n",
    "\n",
    "\n",
    "# from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     space_model.train()  # Set the model to train mode\n",
    "#     t_running_loss = 0.0\n",
    "#     running_train_accuracy=0\n",
    "#     train_accuracy=0\n",
    "    \n",
    "#     train_offensive_normal_loss = 0.0\n",
    "#     train_toxic_intra_loss = 0.0\n",
    "#     train_non_toxic_intra_loss = 0.0\n",
    "\n",
    "    \n",
    "#     # Wrap your dataloader with tqdm\n",
    "#     with tqdm(train_dataloader, desc=f'Epoch {epoch+1}', unit='batch') as train_bar:\n",
    "#         for batch_idx, batch in enumerate(train_bar):\n",
    "#             # print(batch_idx)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             # Move tensors to the same device as the model\n",
    "#             ids = batch['input_ids'].to(device)\n",
    "#             mks = batch['attention_masks'].to(device)\n",
    "#             labels = batch['label'].to(device)\n",
    "#             space = batch['space'].to(device)\n",
    "            \n",
    "#             # Forward pass\n",
    "#             y_logits, loss, off_normal_loss, train_toxic_intra_loss, train_non_toxic_intra_loss = space_model.to(device)(ids, mks, space)\n",
    "     \n",
    "#             labels = labels.float()\n",
    "            \n",
    "#             y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> predicition probabilities -> prediction labels\n",
    "#             # print(y_pred.view(-1,1))\n",
    "\n",
    "\n",
    "#             # 2. Calculate loss/accuracy\n",
    "#             bce_loss = lambda_value*criterion(y_pred.squeeze(), labels)\n",
    "            \n",
    "\n",
    "#             loss+=bce_loss\n",
    "\n",
    "#             train_accuracy = accuracy_fn(y_true=labels, \n",
    "#                               y_pred=y_pred) # for traing wer need  batch accuracy it need to be imrpving for each batch and following epoch\n",
    "\n",
    "#             running_train_accuracy+=train_accuracy\n",
    "#             t_running_loss+=loss.item()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "  \n",
    "            \n",
    "\n",
    "#             train_bar.set_postfix(loss=loss.item(),accuracy=train_accuracy)\n",
    "\n",
    "#     # Validation loop\n",
    "#     space_model.eval()  # Set the model to evaluation mode\n",
    "#     loss=0\n",
    "#     val_loss = 0.0\n",
    "#     total_predictions = 0\n",
    "#     v_running_loss=0\n",
    "#     validation_accuracy=0\n",
    "#     running_validation_accuracy=0\n",
    "\n",
    "    \n",
    "#     val_loss = 0.0\n",
    "#     val_offensive_normal_loss = 0.0\n",
    "#     val_toxic_intra_loss = 0.0\n",
    "#     val_non_toxic_intra_loss = 0.0\n",
    "    \n",
    "#     total_predictions=0.0\n",
    "#     correct_val=0.0\n",
    "\n",
    "    \n",
    "#     with tqdm(val_dataloader, desc=f'Epoch {epoch+1} ', unit='batch') as val_bar:\n",
    "#         with torch.no_grad():\n",
    "#             for batch_idx, batch in enumerate(val_bar):\n",
    "#                 # Move tensors to the same device as the model\n",
    "#                 ids = batch['input_ids'].to(device)\n",
    "#                 mks = batch['attention_masks'].to(device)\n",
    "#                 labels = batch['label'].to(device)\n",
    "#                 space = batch['space'].to(device)\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 y_logits,loss, val_offensive_normal_loss, val_toxic_intra_loss, val_non_toxic_intra_loss = space_model(ids, mks, space)\n",
    "#                 y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "#                 # Calculate loss\n",
    "#                 bce_loss = lambda_value*criterion(y_pred.squeeze(), labels.float())\n",
    "\n",
    "#                 # Calculate accuracy\n",
    "                \n",
    "#                 validation_accuracy = accuracy_fn(y_true=labels, \n",
    "#                               y_pred=y_pred)\n",
    "                \n",
    "#                 # print(validation_accuracy)\n",
    "#                 running_validation_accuracy+=validation_accuracy\n",
    "                \n",
    "#                 correct_predictions = torch.eq(y_pred, labels.view_as(y_pred)).sum().item()\n",
    "#                 # print(correct_predictions)\n",
    "#                 total_predictions += labels.size(0)\n",
    "#                 correct_val += correct_predictions\n",
    "#                 # print(correct_val)\n",
    "#                 loss+=bce_loss\n",
    "                \n",
    "#                 v_running_loss += loss.item()\n",
    "                \n",
    "\n",
    "\n",
    "#                 # Update progress bar\n",
    "#                 val_bar.set_postfix(loss=loss.item() ,accuracy=validation_accuracy)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "#         print(f'Validation_Loss:{100*correct_val/total_predictions:.6f}')\n",
    "\n",
    "#         print(f'Epoch {epoch+1}, Training_Loss: {t_running_loss/len(train_dataloader):.5f}, Train_Accuracy: {running_train_accuracy/len(train_dataloader):.2f}')\n",
    "#         print(f'Validation_Loss: {v_running_loss/len(val_dataloader):.6f},  Val_Accuracy: {running_validation_accuracy/len(val_dataloader):.4f}')\n",
    "\n",
    "\n",
    "\n",
    "# import gc\n",
    "# model = None\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.empty_cache()\n",
    "# print_gpu_utilization(device_id)\n",
    "# torch.cuda.empty_cache()\n",
    "# free_gpu_cache(device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "# model = None\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.empty_cache()\n",
    "# print_gpu_utilization(device_id)\n",
    "# torch.cuda.empty_cache()\n",
    "# free_gpu_cache(device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamter tuning by using Ray tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-03-03 20:58:00</td></tr>\n",
       "<tr><td>Running for: </td><td>00:06:58.74        </td></tr>\n",
       "<tr><td>Memory:      </td><td>85.1/503.5 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 4.000: None | Iter 2.000: -26.097898483276367 | Iter 1.000: -33.03431701660156<br>Logical resource usage: 64.0/64 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:RTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_a75ba_00000</td><td>RUNNING </td><td>192.168.1.206:3865236</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.0376153  </td><td style=\"text-align: right;\">       0.4</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         333.219</td><td style=\"text-align: right;\">26.0979</td></tr>\n",
       "<tr><td>train_model_a75ba_00001</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">0.000495502</td><td style=\"text-align: right;\">       0.4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n",
       "<tr><td>train_model_a75ba_00002</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">0.00010753 </td><td style=\"text-align: right;\">       0.6</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n",
       "<tr><td>train_model_a75ba_00003</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.00047386 </td><td style=\"text-align: right;\">       0.2</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=3865236)\u001b[0m /home/naseem_fordham/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "\u001b[36m(pid=3865236)\u001b[0m   from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "\u001b[36m(train_model pid=3865236)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "\u001b[36m(train_model pid=3865236)\u001b[0m - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(train_model pid=3865236)\u001b[0m - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=3865236)\u001b[0m cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=3865236)\u001b[0m /home/naseem_fordham/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "\u001b[36m(train_model pid=3865236)\u001b[0m   warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "2024-03-03 20:58:00,193\tWARNING tune.py:186 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_3854412/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">96857230.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">152</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_3854412/96857230.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/naseem_fordham/.local/lib/python3.8/site-packages/ray/tune/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tune.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1027</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">run</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1024 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025 │   </span>all_trials = runner.get_trials()                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1026 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1027 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>runner.cleanup()                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1028 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1029 │   </span>incomplete_trials = []                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1030 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> trial <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> all_trials:                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/naseem_fordham/.local/lib/python3.8/site-packages/ray/tune/execution/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tune_controller.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">20</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">17</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">cleanup</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2014 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2015 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">cleanup</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2016 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Cleanup trials and callbacks.\"\"\"</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2017 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._cleanup_trials()                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2018 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.end_experiment_callbacks()                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2019 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2020 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getstate__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/naseem_fordham/.local/lib/python3.8/site-packages/ray/tune/execution/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tune_controller.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">84</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">6</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_cleanup_trials</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 843 │   │   │   │   </span>logger.debug(                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 844 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Waiting for actor manager to clean up final state [dedup]\"</span>           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 845 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 846 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._actor_manager.next(timeout=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 847 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 848 │   │   </span>logger.debug(<span style=\"color: #808000; text-decoration-color: #808000\">\"Force cleanup of remaining actors\"</span>)                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 849 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._cleanup_stopping_actors(force_all=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/naseem_fordham/.local/lib/python3.8/site-packages/ray/air/execution/_internal/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">actor_manage</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">r.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">214</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">next</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">211 │   │   </span>all_futures = resource_futures + shuffled_state_futures + shuffled_task_futures    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">212 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">213 │   │   </span>start_wait = time.monotonic()                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>214 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>ready, _ = ray.wait(all_futures, num_returns=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, timeout=timeout)                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">215 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">216 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> ready:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">217 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/naseem_fordham/.local/lib/python3.8/site-packages/ray/_private/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">auto_init_hook.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">22</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">auto_init_wrapper</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19 │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@wraps</span>(fn)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">auto_init_wrapper</span>(*args, **kwargs):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 │   │   </span>auto_init_ray()                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>22 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> fn(*args, **kwargs)                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23 │   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> auto_init_wrapper                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/naseem_fordham/.local/lib/python3.8/site-packages/ray/_private/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">client_mode_hook.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">103</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">100 │   │   │   # we only convert init function if RAY_CLIENT_MODE=1</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">101 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> func.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span> != <span style=\"color: #808000; text-decoration-color: #808000\">\"init\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> is_client_mode_enabled_by_default:               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">102 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">getattr</span>(ray, func.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span>)(*args, **kwargs)                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>103 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">104 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">105 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> wrapper                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">106 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/naseem_fordham/.local/lib/python3.8/site-packages/ray/_private/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">worker.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2847</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wait</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2844 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2845 │   │   </span>timeout = timeout <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> timeout <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">10</span>**<span style=\"color: #0000ff; text-decoration-color: #0000ff\">6</span>                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2846 │   │   </span>timeout_milliseconds = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>(timeout * <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1000</span>)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2847 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>ready_ids, remaining_ids = worker.core_worker.wait(                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2848 │   │   │   </span>ray_waitables,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2849 │   │   │   </span>num_returns,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2850 │   │   │   </span>timeout_milliseconds,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/naseem_fordham/Pytorch/Xhate/python/ray/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_raylet.pyx</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3604</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">ray._raylet.CoreWorker.wait</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/home/naseem_fordham/Pytorch/Xhate/python/ray/_raylet.pyx'</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/naseem_fordham/Pytorch/Xhate/python/ray/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_raylet.pyx</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">570</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">ray._raylet.check_status</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/home/naseem_fordham/Pytorch/Xhate/python/ray/_raylet.pyx'</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_3854412/\u001b[0m\u001b[1;33m96857230.py\u001b[0m:\u001b[94m152\u001b[0m in \u001b[92m<module>\u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_3854412/96857230.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/naseem_fordham/.local/lib/python3.8/site-packages/ray/tune/\u001b[0m\u001b[1;33mtune.py\u001b[0m:\u001b[94m1027\u001b[0m in \u001b[92mrun\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1024 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1025 \u001b[0m\u001b[2m│   \u001b[0mall_trials = runner.get_trials()                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1026 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1027 \u001b[2m│   \u001b[0mrunner.cleanup()                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1028 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1029 \u001b[0m\u001b[2m│   \u001b[0mincomplete_trials = []                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1030 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m trial \u001b[95min\u001b[0m all_trials:                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/naseem_fordham/.local/lib/python3.8/site-packages/ray/tune/execution/\u001b[0m\u001b[1;33mtune_controller.py\u001b[0m:\u001b[94m20\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m17\u001b[0m in \u001b[92mcleanup\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2014 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2015 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mcleanup\u001b[0m(\u001b[96mself\u001b[0m):                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2016 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Cleanup trials and callbacks.\"\"\"\u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2017 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._cleanup_trials()                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2018 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.end_experiment_callbacks()                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2019 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2020 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__getstate__\u001b[0m(\u001b[96mself\u001b[0m):                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/naseem_fordham/.local/lib/python3.8/site-packages/ray/tune/execution/\u001b[0m\u001b[1;33mtune_controller.py\u001b[0m:\u001b[94m84\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m6\u001b[0m in \u001b[92m_cleanup_trials\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 843 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogger.debug(                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 844 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mWaiting for actor manager to clean up final state [dedup]\u001b[0m\u001b[33m\"\u001b[0m           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 845 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 846 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._actor_manager.next(timeout=\u001b[94m1\u001b[0m)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 847 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 848 \u001b[0m\u001b[2m│   │   \u001b[0mlogger.debug(\u001b[33m\"\u001b[0m\u001b[33mForce cleanup of remaining actors\u001b[0m\u001b[33m\"\u001b[0m)                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 849 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._cleanup_stopping_actors(force_all=\u001b[94mTrue\u001b[0m)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/naseem_fordham/.local/lib/python3.8/site-packages/ray/air/execution/_internal/\u001b[0m\u001b[1;33mactor_manage\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mr.py\u001b[0m:\u001b[94m214\u001b[0m in \u001b[92mnext\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m211 \u001b[0m\u001b[2m│   │   \u001b[0mall_futures = resource_futures + shuffled_state_futures + shuffled_task_futures    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m212 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m213 \u001b[0m\u001b[2m│   │   \u001b[0mstart_wait = time.monotonic()                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m214 \u001b[2m│   │   \u001b[0mready, _ = ray.wait(all_futures, num_returns=\u001b[94m1\u001b[0m, timeout=timeout)                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m215 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m216 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m ready:                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m217 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[94mFalse\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/naseem_fordham/.local/lib/python3.8/site-packages/ray/_private/\u001b[0m\u001b[1;33mauto_init_hook.py\u001b[0m:\u001b[94m22\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mauto_init_wrapper\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@wraps\u001b[0m(fn)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mauto_init_wrapper\u001b[0m(*args, **kwargs):                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   │   \u001b[0mauto_init_ray()                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m22 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m fn(*args, **kwargs)                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m auto_init_wrapper                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/naseem_fordham/.local/lib/python3.8/site-packages/ray/_private/\u001b[0m\u001b[1;33mclient_mode_hook.py\u001b[0m:\u001b[94m103\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mwrapper\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m100 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# we only convert init function if RAY_CLIENT_MODE=1\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m101 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m func.\u001b[91m__name__\u001b[0m != \u001b[33m\"\u001b[0m\u001b[33minit\u001b[0m\u001b[33m\"\u001b[0m \u001b[95mor\u001b[0m is_client_mode_enabled_by_default:               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m102 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mgetattr\u001b[0m(ray, func.\u001b[91m__name__\u001b[0m)(*args, **kwargs)                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m103 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m104 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m105 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m wrapper                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m106 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/naseem_fordham/.local/lib/python3.8/site-packages/ray/_private/\u001b[0m\u001b[1;33mworker.py\u001b[0m:\u001b[94m2847\u001b[0m in \u001b[92mwait\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2844 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2845 \u001b[0m\u001b[2m│   │   \u001b[0mtimeout = timeout \u001b[94mif\u001b[0m timeout \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[94m10\u001b[0m**\u001b[94m6\u001b[0m                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2846 \u001b[0m\u001b[2m│   │   \u001b[0mtimeout_milliseconds = \u001b[96mint\u001b[0m(timeout * \u001b[94m1000\u001b[0m)                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2847 \u001b[2m│   │   \u001b[0mready_ids, remaining_ids = worker.core_worker.wait(                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2848 \u001b[0m\u001b[2m│   │   │   \u001b[0mray_waitables,                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2849 \u001b[0m\u001b[2m│   │   │   \u001b[0mnum_returns,                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2850 \u001b[0m\u001b[2m│   │   │   \u001b[0mtimeout_milliseconds,                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/naseem_fordham/Pytorch/Xhate/python/ray/\u001b[0m\u001b[1;33m_raylet.pyx\u001b[0m:\u001b[94m3604\u001b[0m in \u001b[92mray._raylet.CoreWorker.wait\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/home/naseem_fordham/Pytorch/Xhate/python/ray/_raylet.pyx'\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/naseem_fordham/Pytorch/Xhate/python/ray/\u001b[0m\u001b[1;33m_raylet.pyx\u001b[0m:\u001b[94m570\u001b[0m in \u001b[92mray._raylet.check_status\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/home/naseem_fordham/Pytorch/Xhate/python/ray/_raylet.pyx'\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''  Parallel compitimng code. This model distributes the data accross the GPU's but it comes with caveats. As each GPU performs the calculation\n",
    "individually so combinig them to single will lead to bottle neck. In next cell we are usng DDP'''\n",
    "\n",
    "##### from tqdm import tqdm\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "import ray\n",
    "from ray.train import Checkpoint\n",
    "from ray import train, tune\n",
    "# ray.init(include_dashboard=False)\n",
    "# from ray.experimental.tqdm_ray import tqdm\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "\n",
    "# Set Ray logging level to WARNING\n",
    "ray.init(logging_level=logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(config):\n",
    "    # batch_size = config[\"batch_size\"]\n",
    "    BATCH_SIZE=config[\"batch_size\"]\n",
    "    # print(BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "\n",
    "    # start_epoch = 0\n",
    "    checkpoint = train.get_checkpoint()\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    \n",
    "    ''' Use this if you want to assign it to a multipe device\n",
    "    \n",
    "    In development phase'''\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        \n",
    "        bertModel = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "        bertModel.trainable = False\n",
    "        \n",
    "        train_dataloader, val_dataloader = bert_dataset.dataprep(train_df, val_df, tokenizer, MAX_LEN, BATCH_SIZE, PROJECTION_DIM)\n",
    "\n",
    "        \n",
    "        if torch.cuda.device_count() > 1:\n",
    "            space_model = nn.DataParallel(SpockModel(bertModel, MAX_LEN, PROJECTION_DIM, lambda_value))\n",
    "            print(device)\n",
    "    space_model.to(device)\n",
    "    \n",
    "    ''' Use this if you want to assign it to a single device'''\n",
    "    # space_model = SpockModel(bertModel, MAX_LEN, PROJECTION_DIM, lambda_value).to(device)\n",
    "    optimizer = optim.SGD(space_model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "\n",
    "  \n",
    "\n",
    "    checkpoint = train.get_checkpoint()\n",
    "\n",
    "\n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch = checkpoint_state[\"epoch\"]\n",
    "        space_model.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        space_model.train()\n",
    "        t_running_loss = 0.0\n",
    "        running_train_accuracy = 0\n",
    "        \n",
    "\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            mks = batch['attention_masks'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            space = batch['space'].to(device)\n",
    "      \n",
    "            \n",
    "\n",
    "            y_logits, loss, off_normal_loss, train_toxic_intra_loss, train_non_toxic_intra_loss = space_model(ids, mks, space)\n",
    "        \n",
    "            labels = labels.float()\n",
    "            y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "            bce_loss = lambda_value * criterion(y_pred.squeeze(), labels)\n",
    "            loss += bce_loss\n",
    "\n",
    "            train_accuracy = accuracy_fn(y_true=labels, y_pred=y_pred)\n",
    "            running_train_accuracy += train_accuracy\n",
    "            # print(loss)\n",
    "            loss=torch.mean(loss)\n",
    "            t_running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            metrics = {\"loss\": loss.item()}\n",
    "            \n",
    "            \n",
    "        train.report(metrics,checkpoint=checkpoint)\n",
    "\n",
    "    # Report after the epoch completes\n",
    "    # session.report({\"loss\": v_running_loss / len(val_dataloader), \"accuracy\": correct_val / total_predictions})\n",
    "    # session.report({\"loss\": v_running_loss / len(val_dataloader), \"accuracy\": correct_val / total_predictions})\n",
    "\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "# Define constants\n",
    "MAX_LEN = 4\n",
    "BATCH_SIZE = 8\n",
    "lambda_value = 40\n",
    "num_epochs = 5\n",
    "PROJECTION_DIM = 2\n",
    "num_samples = 4\n",
    "max_num_epochs = 5\n",
    "\n",
    "use_gpu = True  # set this to False to run on CPUs\n",
    "num_workers = 2\n",
    "# Define the configuration for hyperparameter tuning\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"momentum\": tune.choice([0.2,0.4,0.6,0.8]),\n",
    "    \"batch_size\": tune.choice([8, 16, 32]) \n",
    "    \n",
    "}\n",
    "\n",
    "# Configure the ASHA scheduler\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=max_num_epochs,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2,\n",
    "    \n",
    ")\n",
    "\n",
    "result = tune.run(\n",
    "    tune.with_parameters(train_model),\n",
    "    config=config,\n",
    "    resources_per_trial={\"cpu\": 64,\"gpu\": 2},\n",
    "    num_samples=num_samples,\n",
    "    verbose=1,\n",
    "    scheduler=scheduler,\n",
    "    verbose=3,\n",
    "    \n",
    "    \n",
    "    \n",
    "    # resume= \"AUTO\"\n",
    ")\n",
    "\n",
    "# Retrieve and print the best trial information\n",
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "# print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=3845919)\u001b[0m ok\n",
      "\u001b[36m(train_model pid=3845919)\u001b[0m ok\n",
      "\u001b[36m(train_model pid=3845919)\u001b[0m ok\n",
      "\u001b[36m(train_model pid=3845919)\u001b[0m ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  71%|███████   | 688/973 [01:04<00:25, 11.14batch/s]\n"
     ]
    }
   ],
   "source": [
    "'''  Parallel compitimng code. The difference between DistributedDataParallel and DataParallel is: \n",
    "DistributedDataParallel uses multiprocessing where a process is created for each GPU, while DataParallel\n",
    "uses multithreading. By using multiprocessing, each GPU has its dedicated process, \n",
    "this avoids the performance overhead caused by GIL of Python interpreter. '''\n",
    "\n",
    "##### from tqdm import tqdm\n",
    "import torch\n",
    "import ray\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from ray.train import Checkpoint\n",
    "from ray import train, tune\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(config):\n",
    "    # batch_size = config[\"batch_size\"]\n",
    "    BATCH_SIZE=config[\"batch_size\"]\n",
    "    # print(BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "\n",
    "    # start_epoch = 0\n",
    "    checkpoint = train.get_checkpoint()\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    \n",
    "    ''' Use this if you want to assign it to a multipe device\n",
    "    \n",
    "    In development phase'''\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        \n",
    "        bertModel = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "        bertModel.trainable = False\n",
    "        \n",
    "        train_dataloader, val_dataloader = bert_dataset.dataprep(train_df, val_df, tokenizer, MAX_LEN, BATCH_SIZE, PROJECTION_DIM)\n",
    "\n",
    "        \n",
    "        if torch.cuda.device_count() > 1:\n",
    "            space_model = nn.DataParallel(SpockModel(bertModel, MAX_LEN, PROJECTION_DIM, lambda_value))\n",
    "            print(device)\n",
    "    space_model.to(device)\n",
    "    \n",
    "    ''' Use this if you want to assign it to a single device'''\n",
    "    # space_model = SpockModel(bertModel, MAX_LEN, PROJECTION_DIM, lambda_value).to(device)\n",
    "    optimizer = optim.SGD(space_model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "\n",
    "  \n",
    "\n",
    "    checkpoint = train.get_checkpoint()\n",
    "\n",
    "\n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch = checkpoint_state[\"epoch\"]\n",
    "        space_model.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        space_model.train()\n",
    "        t_running_loss = 0.0\n",
    "        running_train_accuracy = 0\n",
    "        \n",
    "\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            mks = batch['attention_masks'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            space = batch['space'].to(device)\n",
    "      \n",
    "            \n",
    "\n",
    "            y_logits, loss, off_normal_loss, train_toxic_intra_loss, train_non_toxic_intra_loss = space_model(ids, mks, space)\n",
    "        \n",
    "            labels = labels.float()\n",
    "            y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "            bce_loss = lambda_value * criterion(y_pred.squeeze(), labels)\n",
    "            loss += bce_loss\n",
    "\n",
    "            train_accuracy = accuracy_fn(y_true=labels, y_pred=y_pred)\n",
    "            running_train_accuracy += train_accuracy\n",
    "            # print(loss)\n",
    "            loss=torch.mean(loss)\n",
    "            t_running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            metrics = {\"loss\": loss.item()}\n",
    "            \n",
    "            \n",
    "        train.report(metrics,checkpoint=checkpoint)\n",
    "\n",
    "    # Report after the epoch completes\n",
    "    # session.report({\"loss\": v_running_loss / len(val_dataloader), \"accuracy\": correct_val / total_predictions})\n",
    "    # session.report({\"loss\": v_running_loss / len(val_dataloader), \"accuracy\": correct_val / total_predictions})\n",
    "\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "# Define constants\n",
    "MAX_LEN = 4\n",
    "BATCH_SIZE = 8\n",
    "lambda_value = 40\n",
    "num_epochs = 5\n",
    "PROJECTION_DIM = 2\n",
    "num_samples = 4\n",
    "max_num_epochs = 5\n",
    "\n",
    "use_gpu = True  # set this to False to run on CPUs\n",
    "num_workers = 2\n",
    "# Define the configuration for hyperparameter tuning\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"momentum\": tune.choice([0.2,0.4,0.6,0.8]),\n",
    "    \"batch_size\": tune.choice([8, 16, 32]) \n",
    "    \n",
    "}\n",
    "\n",
    "# Configure the ASHA scheduler\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=max_num_epochs,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2,\n",
    "    \n",
    ")\n",
    "\n",
    "result = tune.run(\n",
    "    tune.with_parameters(train_model),\n",
    "    config=config,\n",
    "    resources_per_trial={\"cpu\": 64,\"gpu\": 2},\n",
    "    num_samples=num_samples,\n",
    "    verbose=1,\n",
    "    scheduler=scheduler,\n",
    "    verbose=3,\n",
    "    \n",
    "    \n",
    "    \n",
    "    # resume= \"AUTO\"\n",
    ")\n",
    "\n",
    "# Retrieve and print the best trial information\n",
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "# print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### from tqdm import tqdm\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "import ray\n",
    "from ray.train import Checkpoint\n",
    "from ray import train, tune\n",
    "import tempfile\n",
    "import os\n",
    "import warnings\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(config):\n",
    "    # batch_size = config[\"batch_size\"]\n",
    "    BATCH_SIZE=config[\"batch_size\"]\n",
    "    # print(BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    bertModel = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "    bertModel.trainable = False\n",
    "    \n",
    "    # session = train.Session()  # Create a new Session object\n",
    "    # checkpoint = train.get_checkpoint()\n",
    "\n",
    "    # start_epoch = 0\n",
    "    # checkpoint = train.get_checkpoint()\n",
    "    \n",
    "    \n",
    "        # Load existing checkpoint through `get_checkpoint()` API.\n",
    "    if train.get_checkpoint():\n",
    "        loaded_checkpoint = train.get_checkpoint()\n",
    "        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "            model_state, optimizer_state = torch.load(\n",
    "                os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\")\n",
    "            )\n",
    "            net.load_state_dict(model_state)\n",
    "            optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    train_dataloader, val_dataloader = bert_dataset.dataprep(train_df, val_df, tokenizer, MAX_LEN, BATCH_SIZE, PROJECTION_DIM)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # space_model = SpockModel(bertModel, MAX_LEN, PROJECTION_DIM, lambda_value)\n",
    "    \n",
    "    \n",
    "    ''' Use this if you want to assign it to a single device'''\n",
    "    space_model = SpockModel(bertModel, MAX_LEN, PROJECTION_DIM, lambda_value).to(device)\n",
    "    optimizer = optim.SGD(space_model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "\n",
    "  \n",
    "\n",
    "    # checkpoint = train.get_checkpoint()\n",
    "\n",
    "\n",
    "#     if checkpoint:\n",
    "#         checkpoint_state = checkpoint.to_dict()\n",
    "#         start_epoch = checkpoint_state[\"epoch\"]\n",
    "#         space_model.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "#         optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "#     else:\n",
    "#         start_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        space_model.train()\n",
    "        t_running_loss = 0.0\n",
    "        running_train_accuracy = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            mks = batch['attention_masks'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            space = batch['space'].to(device)\n",
    "\n",
    "            y_logits, loss, off_normal_loss, train_toxic_intra_loss, train_non_toxic_intra_loss = space_model(ids, mks, space)\n",
    "            labels = labels.float()\n",
    "            y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "            bce_loss = lambda_value * criterion(y_pred.squeeze(), labels)\n",
    "            loss += bce_loss\n",
    "\n",
    "            train_accuracy = accuracy_fn(y_true=labels, y_pred=y_pred)\n",
    "            running_train_accuracy += train_accuracy\n",
    "            t_running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            metrics = {\"loss\": loss.item()}\n",
    "            \n",
    "            \n",
    "        # train.report(metrics,checkpoint=checkpoint)\n",
    "        \n",
    "\n",
    "        loss=0\n",
    "        val_loss = 0.0\n",
    "        total_predictions = 0\n",
    "        v_running_loss=0\n",
    "        validation_accuracy=0\n",
    "        running_validation_accuracy=0\n",
    "\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_offensive_normal_loss = 0.0\n",
    "        val_toxic_intra_loss = 0.0\n",
    "        val_non_toxic_intra_loss = 0.0\n",
    "\n",
    "        total_predictions=0.0\n",
    "        correct_val=0.0\n",
    "\n",
    "\n",
    "        # with tqdm(val_dataloader, desc=f'Epoch {epoch+1} ', unit='batch') as val_bar:\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_dataloader):\n",
    "                # Move tensors to the same device as the model\n",
    "                ids = batch['input_ids'].to(device)\n",
    "                mks = batch['attention_masks'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                space = batch['space'].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                y_logits,loss, val_offensive_normal_loss, val_toxic_intra_loss, val_non_toxic_intra_loss = space_model(ids, mks, space)\n",
    "                y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "                # Calculate loss\n",
    "                bce_loss = lambda_value*criterion(y_pred.squeeze(), labels.float())\n",
    "\n",
    "                # Calculate accuracy\n",
    "\n",
    "                validation_accuracy = accuracy_fn(y_true=labels, \n",
    "                              y_pred=y_pred)\n",
    "\n",
    "                # print(validation_accuracy)\n",
    "                running_validation_accuracy+=validation_accuracy\n",
    "\n",
    "                correct_predictions = torch.eq(y_pred, labels.view_as(y_pred)).sum().item()\n",
    "                # print(correct_predictions)\n",
    "                total_predictions += labels.size(0)\n",
    "                correct_val += correct_predictions\n",
    "                # print(correct_val)\n",
    "                loss+=bce_loss\n",
    "                v_running_loss += loss.item()\n",
    "\n",
    "#          \n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            path = os.path.join(temp_checkpoint_dir, \"checkpoint.pt\")\n",
    "            torch.save(\n",
    "                (space_model.state_dict(), optimizer.state_dict()), path\n",
    "            )\n",
    "            checkpoint = Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "            train.report(\n",
    "                {\"loss\": v_running_loss/len(val_dataloader), \"accuracy\": running_validation_accuracy/len(val_dataloader)},\n",
    "                checkpoint=checkpoint,\n",
    "            )\n",
    "\n",
    "    # Report after the epoch completes\n",
    "    # session.report({\"loss\": v_running_loss / len(val_dataloader), \"accuracy\": correct_val / total_predictions})\n",
    "    # session.report({\"loss\": v_running_loss / len(val_dataloader), \"accuracy\": correct_val / total_predictions})\n",
    "\n",
    "\n",
    "    print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 18:51:04,400\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2024-03-03 18:51:05,128\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "2024-03-03 18:51:05,128\tINFO tune.py:583 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-03-03 19:16:08</td></tr>\n",
       "<tr><td>Running for: </td><td>00:25:03.29        </td></tr>\n",
       "<tr><td>Memory:      </td><td>90.1/503.5 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=3<br>Bracket: Iter 2.000: -22.511224651630712 | Iter 1.000: -30.270451112939103<br>Logical resource usage: 64.0/64 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:RTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc                  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_e6084_00003</td><td>RUNNING   </td><td>192.168.1.206:3723553</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">4e-05 </td><td style=\"text-align: right;\">       0.6</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_e6084_00004</td><td>PENDING   </td><td>                     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">       0.4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_e6084_00005</td><td>PENDING   </td><td>                     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">       0.4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_e6084_00006</td><td>PENDING   </td><td>                     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">       0.4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_e6084_00007</td><td>PENDING   </td><td>                     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">       0.4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_e6084_00008</td><td>PENDING   </td><td>                     </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">       0.6</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_e6084_00009</td><td>PENDING   </td><td>                     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">       1  </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_e6084_00010</td><td>PENDING   </td><td>                     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">       0.6</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_e6084_00011</td><td>PENDING   </td><td>                     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">4e-05 </td><td style=\"text-align: right;\">       0.8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_e6084_00012</td><td>PENDING   </td><td>                     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">       1  </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_e6084_00013</td><td>PENDING   </td><td>                     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">       1  </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_e6084_00014</td><td>PENDING   </td><td>                     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">       0.4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_model_e6084_00000</td><td>TERMINATED</td><td>192.168.1.206:3720880</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">       1  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">        412.861 </td><td style=\"text-align: right;\">22.4728</td><td style=\"text-align: right;\">   78.156 </td></tr>\n",
       "<tr><td>train_model_e6084_00001</td><td>TERMINATED</td><td>192.168.1.206:3722555</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">       0.4</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.0243</td><td style=\"text-align: right;\">31.719 </td><td style=\"text-align: right;\">   79.5125</td></tr>\n",
       "<tr><td>train_model_e6084_00002</td><td>TERMINATED</td><td>192.168.1.206:3723042</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">       0.8</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         85.8313</td><td style=\"text-align: right;\">30.2705</td><td style=\"text-align: right;\">   66.6282</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 18:51:05,159\tINFO experiment_state.py:402 -- No local checkpoint was found. Starting a new run...\n",
      "\u001b[36m(pid=3720880)\u001b[0m /home/naseem_fordham/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "\u001b[36m(pid=3720880)\u001b[0m   from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "\u001b[36m(train_model pid=3720880)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "\u001b[36m(train_model pid=3720880)\u001b[0m - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(train_model pid=3720880)\u001b[0m - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th style=\"text-align: right;\">  accuracy</th><th>checkpoint_dir_name  </th><th>date               </th><th>done  </th><th>hostname   </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">   loss</th><th>node_ip      </th><th style=\"text-align: right;\">    pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_e6084_00000</td><td style=\"text-align: right;\">   78.156 </td><td>checkpoint_000002    </td><td>2024-03-03_18-58-00</td><td>True  </td><td>lambda-dual</td><td style=\"text-align: right;\">                         3</td><td style=\"text-align: right;\">22.4728</td><td>192.168.1.206</td><td style=\"text-align: right;\">3720880</td><td>True               </td><td style=\"text-align: right;\">            412.861 </td><td style=\"text-align: right;\">          155.336 </td><td style=\"text-align: right;\">      412.861 </td><td style=\"text-align: right;\"> 1709510280</td><td style=\"text-align: right;\">                   3</td><td>e6084_00000</td></tr>\n",
       "<tr><td>train_model_e6084_00001</td><td style=\"text-align: right;\">   79.5125</td><td>checkpoint_000000    </td><td>2024-03-03_18-59-10</td><td>True  </td><td>lambda-dual</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">31.719 </td><td>192.168.1.206</td><td style=\"text-align: right;\">3722555</td><td>True               </td><td style=\"text-align: right;\">             66.0243</td><td style=\"text-align: right;\">           66.0243</td><td style=\"text-align: right;\">       66.0243</td><td style=\"text-align: right;\"> 1709510349</td><td style=\"text-align: right;\">                   1</td><td>e6084_00001</td></tr>\n",
       "<tr><td>train_model_e6084_00002</td><td style=\"text-align: right;\">   66.6282</td><td>checkpoint_000000    </td><td>2024-03-03_19-00-39</td><td>True  </td><td>lambda-dual</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">30.2705</td><td>192.168.1.206</td><td style=\"text-align: right;\">3723042</td><td>True               </td><td style=\"text-align: right;\">             85.8313</td><td style=\"text-align: right;\">           85.8313</td><td style=\"text-align: right;\">       85.8313</td><td style=\"text-align: right;\"> 1709510438</td><td style=\"text-align: right;\">                   1</td><td>e6084_00002</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=3720880)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/naseem_fordham/ray_results/train_model_2024-03-03_18-51-05/train_model_e6084_00000_0_batch_size=8,lr=0.0001,momentum=1.0000_2024-03-03_18-51-05/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=3720880)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/naseem_fordham/ray_results/train_model_2024-03-03_18-51-05/train_model_e6084_00000_0_batch_size=8,lr=0.0001,momentum=1.0000_2024-03-03_18-51-05/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=3720880)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/naseem_fordham/ray_results/train_model_2024-03-03_18-51-05/train_model_e6084_00000_0_batch_size=8,lr=0.0001,momentum=1.0000_2024-03-03_18-51-05/checkpoint_000002)\n",
      "\u001b[36m(pid=3722555)\u001b[0m /home/naseem_fordham/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "\u001b[36m(pid=3722555)\u001b[0m   from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "\u001b[36m(train_model pid=3722555)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "\u001b[36m(train_model pid=3722555)\u001b[0m - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(train_model pid=3722555)\u001b[0m - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36m(train_model pid=3722555)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/naseem_fordham/ray_results/train_model_2024-03-03_18-51-05/train_model_e6084_00001_1_batch_size=32,lr=0.0000,momentum=0.4000_2024-03-03_18-51-05/checkpoint_000000)\n",
      "\u001b[36m(pid=3723042)\u001b[0m /home/naseem_fordham/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "\u001b[36m(pid=3723042)\u001b[0m   from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "\u001b[36m(train_model pid=3723042)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "\u001b[36m(train_model pid=3723042)\u001b[0m - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(train_model pid=3723042)\u001b[0m - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36m(train_model pid=3723042)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/naseem_fordham/ray_results/train_model_2024-03-03_18-51-05/train_model_e6084_00002_2_batch_size=16,lr=0.0001,momentum=0.8000_2024-03-03_18-51-05/checkpoint_000000)\n",
      "\u001b[36m(pid=3723553)\u001b[0m /home/naseem_fordham/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "\u001b[36m(pid=3723553)\u001b[0m   from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "\u001b[36m(train_model pid=3723553)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "\u001b[36m(train_model pid=3723553)\u001b[0m - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36m(train_model pid=3723553)\u001b[0m - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36m(train_model pid=3723553)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/naseem_fordham/ray_results/train_model_2024-03-03_18-51-05/train_model_e6084_00003_3_batch_size=16,lr=0.0000,momentum=0.6000_2024-03-03_18-51-05/checkpoint_000000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define constants\n",
    "MAX_LEN = 25\n",
    "# BATCH_SIZE = 8\n",
    "lambda_value = 40\n",
    "num_epochs = 4\n",
    "PROJECTION_DIM = 10\n",
    "num_samples = 15\n",
    "max_num_epochs = 3\n",
    "\n",
    "use_gpu = True  # set this to False to run on CPUs\n",
    "num_workers = 2\n",
    "# Define the configuration for hyperparameter tuning\n",
    "config = {\n",
    "    # \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"lr\": tune.choice([1e-4,1e-5,1e-3,4e-5]),\n",
    "    \"momentum\": tune.choice([0.4,0.6,0.8,1.0]),\n",
    "    \"batch_size\": tune.choice([8,16,32,64]) \n",
    "    \n",
    "}\n",
    "\n",
    "# Configure the ASHA scheduler\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=max_num_epochs,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "result = tune.run(\n",
    "    tune.with_parameters(train_model),\n",
    "    config=config,\n",
    "    resources_per_trial={\"cpu\": 64,\"gpu\": 2},\n",
    "    num_samples=num_samples,\n",
    "    verbose=3,\n",
    "    scheduler=scheduler,\n",
    "    resume= \"AUTO\"\n",
    ")\n",
    "\n",
    "# Retrieve and print the best trial information\n",
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "# print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hyperparamter tuning by using Raytune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Import tqdm\n",
    "from ray import tune\n",
    "# from ray.air import Checkpoint, session\n",
    "from ray import train, tune\n",
    "from ray.train import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "\n",
    "# train_dataloader,val_dataloader = bert_dataset.dataprep(train_df, val_df,tokenizer, MAX_LEN, BATCH_SIZE,PROJECTION_DIM)\n",
    "# space_model = SpockModel(MAX_LEN, PROJECTION_DIM, lambda_value).to(device)\n",
    "\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # BATCH_SIZE=config[\"batch_size\"]\n",
    "    # print(BATCH_SIZE)\n",
    "    \n",
    "    # load bertModel, bertTokenizer and freeze all layers\n",
    "    bertModel = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "\n",
    "    bertModel.trainable = False # freezing the weights\n",
    "    \n",
    "\n",
    "\n",
    "    train_dataloader,val_dataloader = bert_dataset.dataprep(train_df, val_df,tokenizer, MAX_LEN, BATCH_SIZE,PROJECTION_DIM)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    space_model = SpockModel(MAX_LEN, PROJECTION_DIM, lambda_value).to(device)\n",
    " \n",
    "\n",
    "    optimizer = optim.SGD(space_model.parameters(), lr=config[\"lr\"],momentum=config[\"momentum\"])\n",
    "    \n",
    "    checkpoint = session.get_checkpoint()\n",
    "\n",
    "\n",
    "\n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch = checkpoint_state[\"epoch\"]\n",
    "        space_model.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        space_model.train()  # Set the model to train mode\n",
    "        t_running_loss = 0.0\n",
    "        running_train_accuracy=0\n",
    "        train_accuracy=0\n",
    "\n",
    "        train_offensive_normal_loss = 0.0\n",
    "        train_toxic_intra_loss = 0.0\n",
    "        train_non_toxic_intra_loss = 0.0\n",
    "\n",
    "\n",
    "        # Wrap your dataloader with tqdm\n",
    "        with tqdm(train_dataloader, desc=f'Epoch {epoch+1}', unit='batch') as train_bar:\n",
    "            for batch_idx, batch in enumerate(train_bar):\n",
    "                # print(batch_idx)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Move tensors to the same device as the model\n",
    "                ids = batch['input_ids'].to(device)\n",
    "                mks = batch['attention_masks'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                space = batch['space'].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                y_logits, loss, off_normal_loss, train_toxic_intra_loss, train_non_toxic_intra_loss = space_model.to(device)(ids, mks, space)\n",
    "\n",
    "                labels = labels.float()\n",
    "\n",
    "                y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> predicition probabilities -> prediction labels\n",
    "                # print(y_pred.view(-1,1))\n",
    "\n",
    "\n",
    "                # 2. Calculate loss/accuracy\n",
    "                bce_loss = lambda_value*criterion(y_pred.squeeze(), labels)\n",
    "\n",
    "\n",
    "                loss+=bce_loss\n",
    "\n",
    "                train_accuracy = accuracy_fn(y_true=labels, \n",
    "                                  y_pred=y_pred) # for traing wer need  batch accuracy it need to be imrpving for each batch and following epoch\n",
    "\n",
    "                running_train_accuracy+=train_accuracy\n",
    "                t_running_loss+=loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                train_bar.set_postfix(loss=loss.item(),accuracy=train_accuracy)\n",
    "\n",
    "                # Validation loop\n",
    "        space_model.eval()  # Set the model to evaluation mode\n",
    "        loss=0\n",
    "        val_loss = 0.0\n",
    "        total_predictions = 0\n",
    "        v_running_loss=0\n",
    "        validation_accuracy=0\n",
    "        running_validation_accuracy=0\n",
    "\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_offensive_normal_loss = 0.0\n",
    "        val_toxic_intra_loss = 0.0\n",
    "        val_non_toxic_intra_loss = 0.0\n",
    "\n",
    "        total_predictions=0.0\n",
    "        correct_val=0.0\n",
    "\n",
    "\n",
    "        with tqdm(val_dataloader, desc=f'Epoch {epoch+1} ', unit='batch') as val_bar:\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, batch in enumerate(val_bar):\n",
    "                    # Move tensors to the same device as the model\n",
    "                    ids = batch['input_ids'].to(device)\n",
    "                    mks = batch['attention_masks'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "                    space = batch['space'].to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    y_logits,loss, val_offensive_normal_loss, val_toxic_intra_loss, val_non_toxic_intra_loss = space_model(ids, mks, space)\n",
    "                    y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "                    # Calculate loss\n",
    "                    bce_loss = lambda_value*criterion(y_pred.squeeze(), labels.float())\n",
    "\n",
    "                    # Calculate accuracy\n",
    "\n",
    "                    validation_accuracy = accuracy_fn(y_true=labels, \n",
    "                                  y_pred=y_pred)\n",
    "\n",
    "                    # print(validation_accuracy)\n",
    "                    running_validation_accuracy+=validation_accuracy\n",
    "\n",
    "                    correct_predictions = torch.eq(y_pred, labels.view_as(y_pred)).sum().item()\n",
    "                    # print(correct_predictions)\n",
    "                    total_predictions += labels.size(0)\n",
    "                    correct_val += correct_predictions\n",
    "                    # print(correct_val)\n",
    "                    loss+=bce_loss\n",
    "\n",
    "                    v_running_loss += loss.item()\n",
    "\n",
    "                    # Update progress bar\n",
    "                    val_bar.set_postfix(loss=loss.item() ,accuracy=validation_accuracy)\n",
    "                    \n",
    "                    \n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"net_state_dict\": net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        checkpoint = Checkpoint.from_dict(checkpoint_data)\n",
    "\n",
    "        session.report(\n",
    "            {\"loss\": v_running_loss/len(val_dataloader), \"accuracy\": correct_val / total_predictions},\n",
    "            checkpoint=checkpoint,\n",
    "        )\n",
    "        print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "\n",
    "import ray\n",
    "MAX_LEN = 4\n",
    "BATCH_SIZE = 8\n",
    "lambda_value = 40\n",
    "# num_epochs = 2\n",
    "PROJECTION_DIM = 2\n",
    "num_samples = 1\n",
    "max_num_epochs = 2\n",
    "\n",
    "# ray.init(num_cpus=123, num_gpus=2)\n",
    "use_gpu = True  # set this to False to run on CPUs\n",
    "num_workers = 1  # set this to number of GPUs or CPUs you want to use\n",
    "\n",
    "# Define the configuration for hyperparameter tuning\n",
    "config = {\n",
    "    \"lr\": tune.choice([0.0005,0.004]),\n",
    "    \"momentum\": tune.choice([0.2]),\n",
    "}\n",
    "\n",
    "# Configure the ASHA scheduler\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=max_num_epochs,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2,\n",
    ")\n",
    "\n",
    "result = tune.run(\n",
    "    tune.with_parameters(train_model),\n",
    "    # train_model,\n",
    "    config=config,\n",
    "    resources_per_trial={\"cpu\": 12,\"gpu\": 2},\n",
    "    num_samples=num_samples,\n",
    "    verbose=2,\n",
    "    scheduler=scheduler,\n",
    "    resume=\"AUTO\"\n",
    ")\n",
    "\n",
    "# Retrieve and print the best trial information\n",
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "!pip freeze | grep grpcio\n",
    "# grpcio==1.48.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install  grpcio==1.41.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip freeze |grep ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "model = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "print_gpu_utilization(device_id)\n",
    "torch.cuda.empty_cache()\n",
    "free_gpu_cache(device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# New Fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dataloader, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_offensive_normal_loss = 0.0\n",
    "    train_toxic_intra_loss = 0.0\n",
    "    train_non_toxic_intra_loss = 0.0\n",
    "        \n",
    "    train_preds_labels=[]\n",
    "    train_labels=[]\n",
    "\n",
    "\n",
    "\n",
    "    with tqdm(train_dataloader, desc=f'Epoch {epoch} ', unit='batch') as train_bar:\n",
    "\n",
    "        for batch_idx,batch in enumerate(train_bar):\n",
    "            \n",
    "                 \n",
    "\n",
    "        \n",
    "            # loss=0\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            mks = batch['attention_masks'].to(device)\n",
    "            space = batch['space'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "          \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_logits, loss, off_normal_loss, toxic_intra_loss, non_toxic_intra_loss = model(ids, mks, space)\n",
    "            \n",
    "            predictions = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "            # Compute Binary Cross-Entropy Loss\n",
    "            # bce_loss = torch.nn.BCELoss()(predictions, labels.float())\n",
    "            \n",
    "            bce_loss = torch.nn.BCEWithLogitsLoss()(predictions.view(-1,1), labels.float().view(-1,1))\n",
    "\n",
    "            # Total Loss\n",
    "            loss += bce_loss\n",
    "            train_accuracy = accuracy_fn(y_true=labels, \n",
    "                              y_pred=predictions)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_offensive_normal_loss += off_normal_loss.item()\n",
    "            train_toxic_intra_loss += toxic_intra_loss.item()\n",
    "            train_non_toxic_intra_loss += non_toxic_intra_loss.item()\n",
    "            \n",
    "                        \n",
    "            train_preds_labels+=y_pred.detach().tolist()\n",
    "            train_labels += [l.item() for l in labels]\n",
    "            \n",
    "            \n",
    "            train_bar.set_postfix(loss=train_loss / (batch_idx + 1),accuracy=train_accuracy)\n",
    "            \n",
    "            \n",
    "        # print(accuracy_score(train_labels, train_preds_labels))\n",
    "\n",
    "\n",
    "        return train_loss, predictions, labels, train_offensive_normal_loss, train_toxic_intra_loss, train_non_toxic_intra_loss\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_offensive_normal_loss = 0.0\n",
    "    val_toxic_intra_loss = 0.0\n",
    "    val_non_toxic_intra_loss = 0.0\n",
    "\n",
    "    with tqdm(val_dataloader, desc=f'Epoch {epoch} ', unit='batch') as val_bar:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_bar):\n",
    "                ids = batch['input_ids'].to(device)\n",
    "                mks = batch['attention_masks'].to(device)\n",
    "                space = batch['space'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                y_logits, loss, off_normal_loss, toxic_intra_loss, non_toxic_intra_loss = model(ids, mks, space)\n",
    "\n",
    "                # Compute Binary Cross-Entropy Loss\n",
    "                # bce_loss = torch.nn.BCELoss()(predictions, labels.float())\n",
    "                \n",
    "                predictions = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "                # Compute Binary Cross-Entropy Loss\n",
    "                # bce_loss = torch.nn.BCELoss()(predictions, labels.float())\n",
    "\n",
    "                bce_loss = torch.nn.BCEWithLogitsLoss()(predictions.view(-1,1), labels.float().view(-1,1))\n",
    "\n",
    "                # Total Loss\n",
    "                loss += bce_loss\n",
    "                val_accuracy = accuracy_fn(y_true=labels, \n",
    "                                  y_pred=predictions)\n",
    "\n",
    "\n",
    "\n",
    "                # Total Loss\n",
    "                loss += bce_loss\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_offensive_normal_loss += off_normal_loss.item()\n",
    "                val_toxic_intra_loss += toxic_intra_loss.item()\n",
    "                val_non_toxic_intra_loss += non_toxic_intra_loss.item()\n",
    "                \n",
    "                val_bar.set_postfix(loss=val_loss / (batch_idx + 1),accuracy=val_accuracy)\n",
    "            \n",
    "     \n",
    "\n",
    "    return val_loss, predictions, labels, val_offensive_normal_loss, val_toxic_intra_loss, val_non_toxic_intra_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def eval(f):\n",
    "#     def wrapper(model, *args, **kwargs):\n",
    "#         model.eval()\n",
    "#         return f(model, *args, **kwargs)\n",
    "#     return wrapper\n",
    "\n",
    "# def train(f):\n",
    "#     def wrapper(model, *args, **kwargs):\n",
    "#         model.train()\n",
    "#         return f(model, *args, **kwargs)\n",
    "#     return wrapper\n",
    "\n",
    "# # train_dataloader,val_dataloader=dataprep(3)\n",
    "# @train\n",
    "# def train_epoch(model, train_dataloader, optimizer):\n",
    "#     train_loss = 0.0\n",
    "#     train_preds = []\n",
    "#     train_labels = []\n",
    "\n",
    "#     for step, batch in enumerate(train_dataloader):\n",
    "#         # print(step)\n",
    "#         # if step==1:\n",
    "#         # loss=0\n",
    "        \n",
    "        \n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_masks = batch['attention_masks'].to(device)\n",
    "#         space = batch['space'].to(device)\n",
    "#         labels = batch['label'].to(device)\n",
    "#         outputs = model(input_ids, attention_masks, space) # (B, Seq_Len, 2)\n",
    "#         preds,loss,train_offensive_normal_loss,train_toxic_intra_loss, train_non_toxic_intra_loss= outputs\n",
    "        \n",
    "#         labels = labels.view(-1, 1).float()\n",
    "        \n",
    "#         bce_loss = nn.BCEWithLogitsLoss()(preds.view(-1,1), labels.float())\n",
    "#         loss += bce_loss\n",
    "        \n",
    "#         train_preds += preds.detach().tolist()\n",
    "#         # print(preds)\n",
    "#         train_labels += [l.item() for l in labels]\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         loss.backward()\n",
    "                        \n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_loss += loss.item()\n",
    "#         # else:\n",
    "#         #     break\n",
    "#     return train_loss, train_preds, train_labels,train_offensive_normal_loss,train_toxic_intra_loss, train_non_toxic_intra_loss\n",
    "\n",
    "# @eval\n",
    "# # '''PLOT HISTIOGRAM TO SEE THE SCORE DISTRIBUTION'''\n",
    "\n",
    "# def eval_epoch(model, val_dataloader):\n",
    "#     val_loss = 0.0\n",
    "#     val_preds = []\n",
    "#     val_labels = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "\n",
    "#         for step, batch in enumerate(val_dataloader):\n",
    "#             # if step==1:\n",
    "#             input_ids = batch['input_ids'].to(device)\n",
    "#             attention_masks = batch['attention_masks'].to(device)\n",
    "#             space = batch['space'].to(device)\n",
    "#             labels = batch['label']\n",
    "\n",
    "#             outputs = model(input_ids, attention_masks, space) # (B, Seq_Len, 2)\n",
    "\n",
    "#             # loss, logits = outputs.loss, outputs.logits\n",
    "#             preds,loss,val_offensive_normal_loss, val_toxic_intra_loss, val_non_toxic_intra_loss = outputs\n",
    "#             loss=torch.mean(loss)\n",
    "#             # print(preds.dtype)\n",
    "\n",
    "\n",
    "#             # probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "#             # pred = torch.argmax(probs, dim=-1) # (B)\n",
    "#             val_preds += preds.detach().tolist()\n",
    "#             val_labels += [l.item() for l in labels]\n",
    "#             # print(val_labels,val_preds)\n",
    "#             val_loss += loss.item()\n",
    "#         # else:\n",
    "#         #     continue\n",
    "\n",
    "               \n",
    "    # return val_loss, val_preds, val_labels,val_offensive_normal_loss, val_toxic_intra_loss, val_non_toxic_intra_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Metrices\n",
    "#assert take afucntion to take a conditon \n",
    "\n",
    "\n",
    "def training(model, train_data, val_data, config):\n",
    "    model = model\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "\n",
    "    # optimizer = torch.optim.Adam(\n",
    "    #     params=model.parameters(),\n",
    "    #     lr=config['lr'],\n",
    "        # weight_decay=config['weight_decay']\n",
    "    # )\n",
    "\n",
    "    num_train_steps = int(len(train_df) / (config['batch_size'] * config['epochs']))\n",
    "    # num_train_steps=2\n",
    "\n",
    "    print(f'Train steps: {num_train_steps}')\n",
    "\n",
    "\n",
    "    history = {\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'train_f1': [],\n",
    "        'val_f1': [],\n",
    "        'train_precision': [],\n",
    "        'val_precision': [],\n",
    "        'train_recall': [],\n",
    "        'val_recall': [],\n",
    "        'train_offensive_normal_loss': [],\n",
    "        'train_toxic_intra_loss': [],\n",
    "        'train_non_toxic_intra_loss': [],\n",
    "        'val_offensive_normal_loss': [],\n",
    "        'val_toxic_intra_loss': [],\n",
    "        'val_non_toxic_intra_loss': [],\n",
    "    }\n",
    "    # Inside the training loop\n",
    "    for epoch_num in range(config['epochs']):\n",
    "        print(f'Epoch: {epoch_num }')\n",
    "\n",
    "        # Train stage\n",
    "        train_loss, train_preds, train_labels,train_offensive_normal_loss,train_toxic_intra_loss, train_non_toxic_intra_loss = train_epoch(model, train_dataloader, optimizer)\n",
    "        \n",
    "\n",
    "        # Eval stage\n",
    "        val_loss, val_preds, val_labels,val_offensive_normal_loss, val_toxic_intra_loss, val_non_toxic_intra_loss= evaluate(model, val_dataloader)\n",
    "\n",
    "\n",
    "        train_preds_tensor = torch.tensor(train_preds).detach()\n",
    "        val_preds_tensor = torch.tensor(val_preds).detach()\n",
    "        \n",
    "        \n",
    "        train_preds_labels = train_preds_tensor.cpu().numpy()\n",
    "        val_preds_labels = val_preds_tensor.cpu().numpy()\n",
    "        \n",
    "        val_labels=val_labels.cpu().numpy()\n",
    "        train_labels=train_labels.cpu().numpy()\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "        # Metrics calculation\n",
    "        train_acc = accuracy_score(train_labels, train_preds_labels)\n",
    "        val_acc = accuracy_score(val_labels, val_preds_labels)\n",
    "        train_f1 = f1_score(train_labels, train_preds_labels, average='macro')\n",
    "        val_f1 = f1_score(val_labels, val_preds_labels, average='macro')\n",
    "        train_precision = precision_score(train_labels, train_preds_labels, average='weighted')\n",
    "        val_precision = precision_score(val_labels, val_preds_labels, average='weighted')\n",
    "        train_recall = recall_score(train_labels, train_preds_labels, average='weighted')\n",
    "        val_recall = recall_score(val_labels, val_preds_labels, average='weighted')\n",
    "\n",
    "        # Update history dictionary\n",
    "        history['train_losses'].append(train_loss / len(train_dataloader))\n",
    "        history['val_losses'].append(val_loss / len(val_dataloader))\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_f1'].append(train_f1)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['train_precision'].append(train_precision)\n",
    "        history['val_precision'].append(val_precision)\n",
    "        history['train_recall'].append(train_recall)\n",
    "        history['val_recall'].append(val_recall)\n",
    "        \n",
    "        history['train_offensive_normal_loss'].append(train_offensive_normal_loss)\n",
    "        history['train_toxic_intra_loss'].append(train_toxic_intra_loss)\n",
    "        history['train_non_toxic_intra_loss'].append(train_non_toxic_intra_loss)\n",
    "\n",
    "        # Append validation loss values\n",
    "        history['val_offensive_normal_loss'].append(val_offensive_normal_loss)\n",
    "        history['val_toxic_intra_loss'].append(val_toxic_intra_loss)\n",
    "        history['val_non_toxic_intra_loss'].append(val_non_toxic_intra_loss)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "#         history['train_offensive_normal_loss'].append(train_offensive_normal_loss.detach().cpu().numpy())\n",
    "#         history['train_toxic_intra_loss'].append(train_toxic_intra_loss.detach().cpu().numpy())\n",
    "#         history['train_non_toxic_intra_loss'].append(train_non_toxic_intra_loss.detach().cpu().numpy())\n",
    "\n",
    "#         # Append validation loss values\n",
    "#         history['val_offensive_normal_loss'].append(val_offensive_normal_loss.detach().cpu().numpy())\n",
    "#         history['val_toxic_intra_loss'].append(val_toxic_intra_loss.detach().cpu().numpy())\n",
    "#         history['val_non_toxic_intra_loss'].append(val_non_toxic_intra_loss.detach().cpu().numpy())\n",
    "\n",
    "        print()\n",
    "        print(f'Train loss: {train_loss / len(train_dataloader)} | Val loss: {val_loss / len(val_dataloader)}')\n",
    "        print(f'Train acc: {train_acc} | Val acc: {val_acc}')\n",
    "        # print(f'Train f1: {train_f1} | Val f1: {val_f1}')\n",
    "        # print(f'Train precision: {train_precision} | Val precision: {val_precision}')\n",
    "        # print(f'Train recall: {train_recall} | Val recall: {val_recall}')\n",
    "        \n",
    "        \n",
    "\n",
    "    # Free GPU cache if necessary\n",
    "    free_gpu_cache(device_id)\n",
    "\n",
    "    return history,val_preds, val_labels,train_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_gpu_utilization(device_id)\n",
    "torch.cuda.empty_cache()\n",
    "free_gpu_cache(device_id)\n",
    "MAX_LEN = 50\n",
    "PROJECTION_DIM = 10\n",
    "BATCH_SIZE=16\n",
    "w1=0.5\n",
    "w2=0.5\n",
    "# VECTOR_DIM = 768\n",
    "lambda_value = 0.2\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE=0.001\n",
    "# Initialize model\n",
    "\n",
    "spock_model = SpockModel(MAX_LEN, PROJECTION_DIM, lambda_value)\n",
    "# spock_model.to(device)\n",
    "\n",
    "\n",
    "# train_dataloader,val_dataloader=dataprep(PROJECTION_DIM)\n",
    "# Call the dataprep method on the object\n",
    "train_dataloader,val_dataloader = bert_dataset.dataprep(train_df, val_df, tokenizer, MAX_LEN, BATCH_SIZE, PROJECTION_DIM)\n",
    "\n",
    "config = {\n",
    "    'epochs': NUM_EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'val_batch_size': BATCH_SIZE,\n",
    "    \n",
    "    'fp16': False,\n",
    "    'lr': LEARNING_RATE,\n",
    "}\n",
    "history,val_preds, val_labels,train_preds = training(spock_model.to(device), train_dataloader, val_dataloader, config)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_results(history, do_val=True):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    # Losses\n",
    "    axs[0, 0].plot(history['train_losses'], label='Train Loss')\n",
    "    if do_val:\n",
    "        axs[0, 0].plot(history['val_losses'], label='Validation Loss')\n",
    "    axs[0, 0].set_title('Train / Validation Loss')\n",
    "    axs[0, 0].set_xlabel('Epochs')\n",
    "    axs[0, 0].set_ylabel('Loss')\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    # Accuracy\n",
    "    axs[0, 1].plot(history['train_acc'], label='Train Accuracy')\n",
    "    if do_val:\n",
    "        axs[0, 1].plot(history['val_acc'], label='Validation Accuracy')\n",
    "    axs[0, 1].set_title('Train / Validation Accuracy')\n",
    "    axs[0, 1].set_xlabel('Epochs')\n",
    "    axs[0, 1].set_ylabel('Accuracy')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    # F1 Score\n",
    "    # axs[1, 0].plot(history['train_f1'], label='Train F1 Score')\n",
    "    \n",
    "    axs[1, 0].plot(history['train_offensive_normal_loss'], label='train_offensive_normal_loss')\n",
    "    if do_val:\n",
    "        axs[1, 0].plot(history['val_offensive_normal_loss'], label='val_offensive_normal_loss')\n",
    "\n",
    "    axs[1, 0].set_title('Train / Validation offensive_normal_loss')\n",
    "    axs[1, 0].set_xlabel('Epochs')\n",
    "    axs[1, 0].set_ylabel('offensive_normal_loss')\n",
    "    axs[1, 0].legend()\n",
    "\n",
    "    # Precision\n",
    "    \n",
    "    axs[1,1].plot(history['train_toxic_intra_loss'], label='train_toxic_intra_loss')\n",
    "\n",
    "    if do_val:\n",
    "        axs[1,1].plot(history['train_non_toxic_intra_loss'], label='train_non_toxic_intra_loss')\n",
    "    # axs[1, 1].plot(history['train_precision'], label='Train Precision')\n",
    "    # if do_val:\n",
    "    #     axs[1, 1].plot(history['val_precision'], label='Validation Precision')\n",
    "    axs[1, 1].set_title('Toxic/ Non_Toxix intra_loss')\n",
    "    axs[1, 1].set_xlabel('Epochs')\n",
    "    axs[1, 1].set_ylabel('Intra_loss')\n",
    "    axs[1, 1].legend()\n",
    "    \n",
    "\n",
    "    \n",
    "    plt.subplots_adjust(wspace=5, hspace=0.5)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_results(history)\n",
    "\n",
    "# learn oythin debggung\n",
    "#ASSERT FUNCTION\n",
    "#USE ATMOST 100 LABELS\n",
    "#PLOT MIRE SENSISBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "OJHKxhG4Uphm",
    "outputId": "8e989294-bc36-4224-f212-269241763980",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# Set up hyperparameters\n",
    "MAX_LENGTH = 30\n",
    "BATCH_SIZE = 16\n",
    "PROJECTION_DIM = 30\n",
    "VECTOR_DIM = 768\n",
    "lambda_value = 2\n",
    "EPOCHS = 20\n",
    "PROJECTION_DIM = 10  # You redefined PROJECTION_DIM here, it might not be necessary\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "spock_model1 = SpockModel(MAX_LENGTH, PROJECTION_DIM, lambda_value)\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Define your loss function\n",
    "optimizer = optim.Adam(spock_model1.parameters(), lr=0.001)  # Define your optimizer\n",
    "train_ds,val_ds=dataprep(PROJECTION_DIM)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i,data in enumerate(train_ds):\n",
    "        # Get the inputs, attention masks, space, attention score, and labels from the dictionary\n",
    "        input_ids = data['input_ids']\n",
    "        attention_masks = data['attention_masks']\n",
    "        space = data['space']\n",
    "        labels = data['label']\n",
    "        \n",
    "        \n",
    "        # print(input_ids.shape,attention_masks.shape,space.shape,attention_score.shape)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # print('ok')\n",
    "\n",
    "        # Forward pass\\ids, mks, projection_space, attention_score\n",
    "        outputs, loss = spock_model1(input_ids, attention_masks, space)\n",
    "        # print('ok3')\n",
    "\n",
    "        # Calculate loss\n",
    "        \n",
    "        labels=labels.view(-1,1)\n",
    "        # print(outputs.dtype,labels.dtype)\n",
    "        # print(outputs.shape,labels.shape)\n",
    "        labels = labels.float()\n",
    "    \n",
    "        loss = criterion(outputs, labels)\n",
    "        # print('ok')\n",
    "\n",
    "        # Backward pass and optimize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print('ok')\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 5== 1999:  # Print every 2000 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 2000:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Log loss to TensorBoard\n",
    "    writer.add_scalar(\"training_loss\", running_loss, epoch)\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiazliz the tensorboard to visualize the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1st laod the teansorbaord before tarining\n",
    "# %load_ext tensorboard\n",
    "# %reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAX_LENGTH = 30\n",
    "# BATCH_SIZE = 16\n",
    "# PROJECTION_DIM =30\n",
    "# VECTOR_DIM = 768\n",
    "# \n",
    "# spock_model1= spock_model(MAX_LENGTH, PROJECTION_DIM)\n",
    "# spock_model1.summary()\\\n",
    "# tf.keras.backend.clear_session()\n",
    "# print(\"TensorFlow version: \", tf.__version__)\n",
    "\n",
    "# # Load the TensorBoard notebook extension.\n",
    "# %load_ext tensorboard\n",
    "# %reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !ps aux | grep tensorboard\n",
    "# # !kill 146651\n",
    "# !pkill -f tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.profiler.experimental.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating Model and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30\n",
    "BATCH_SIZE = 32\n",
    "# PROJECTION_DIM =30\n",
    "VECTOR_DIM = 768\n",
    "lambda_value=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ##incase the port is already using, we have to kill it forst\n",
    "# !lsof -i :6006\n",
    "# !kill 148692\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#laod the tensor board with passing the base path. Note incase of runing on VPN, I need to \n",
    "# http://192.168.1.206:8000/user/naseem_fordham/proxy/6006/ where 6006 is the local host port\n",
    "# %tensorboard --logdir $BASE_PATH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#accuracy plots\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def plot(history,path):\n",
    "    # Create a new figure for the combined plot\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Plot accuracy on the first subplot\n",
    "    axs[0].plot(history['accuracy'])\n",
    "    axs[0].plot(history['val_accuracy'])\n",
    "    axs[0].set_title('Model Accuracy', fontsize=12)\n",
    "    axs[0].set_ylim(0, 1, 0.1)\n",
    "    axs[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axs[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axs[0].legend(['Train', 'Validation'], loc='upper left', fontsize=12)\n",
    "    \n",
    "    # Plot loss on the second subplot\n",
    "\n",
    "\n",
    "    axs[1].plot(history['loss'])\n",
    "    axs[1].plot(history['val_loss'])\n",
    "    axs[1].plot(history['Interspace'])\n",
    "    axs[1].plot(history['ToxicIntra_Loss'])\n",
    "    axs[1].plot(history['Non_toxicIntra_Loss'])\n",
    "    axs[1].plot(history['Attention_Loss'])\n",
    "    \n",
    " \n",
    "\n",
    "    axs[1].set_title('Model Loss', fontsize=12)\n",
    "    axs[1].set_ylim(0, 15, 1)\n",
    "    axs[1].set_ylabel('Loss', fontsize=12)\n",
    "    axs[1].set_xlabel('Epoch', fontsize=12)\n",
    "    # axs[1].legend(['Train', 'Validation','offensive_normal_loss','posStdDevLoss','norStdDevLoss'], loc='upper left', fontsize=12)\n",
    "    # axs[1].legend(['Train', 'Validation','offensive_normal_loss'], loc='upper left', fontsize=12)\n",
    "    \n",
    "    axs[1].legend(['Train', 'Validation','Interspace','ToxicIntra_Loss','Non_toxicIntra_Loss','Attention_loss'], loc='upper left', fontsize=12)\n",
    "    # axs[1].legend(['Train', 'Validation','interspace'], loc='upper left', fontsize=12)\n",
    "\n",
    "    # axs[1].legend(['Train', 'Validation','ToxicIntra_Loss','Non_toxicIntra_Loss','interspace'], loc='upper left', fontsize=12)\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "    # Adjust spacing between subplots\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "    # Save the combined plot as a single image\n",
    "    plt.savefig(path, dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model laoding from directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert sigmoid outputs to labels\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def predictionLabels(i):\n",
    "    return np.where(i < 0.5, 0.0, 1.0)\n",
    "\n",
    "    \n",
    "    \n",
    "    # return np.argmax(i, axis=1)\n",
    "\n",
    "pattern = r\"_([0-9]+)$\"\n",
    "\n",
    "\n",
    "getLabels = np.vectorize(predictionLabels)\n",
    "# predictions = model.predict(test_ds)\n",
    "# predictedLabels = getLabels(predictions)\n",
    "\n",
    "BASE_PATH = f\"/home/naseem_fordham/Spock-paper/Spock_Hateoffensive/\"\n",
    "\n",
    "accuracy=[]\n",
    "# Iterate over subdirectories\n",
    "for folder_name in os.listdir(BASE_PATH):\n",
    "    print(folder_name)\n",
    "    folder_path = os.path.join(BASE_PATH, folder_name)\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        match = re.search(pattern, folder_name)\n",
    "        if match:\n",
    "            model_number = match.group(1)\n",
    "            print(folder_path)\n",
    "            spock_model2 = spock_model(MAX_LENGTH, int(model_number),lambda_value)\n",
    "            model_filename = os.path.join(folder_path, f\"CS_{model_number}.h5\")\n",
    "            print(model_filename)\n",
    "\n",
    "            if os.path.exists(model_filename):\n",
    "                spock_model2.load_weights(model_filename)\n",
    "                \n",
    "                history=np.load(f\"{folder_path}/training_history{model_number}.pkl\",allow_pickle=True)\n",
    "                accuracy.append(history['val_accuracy'][-1])\n",
    "                \n",
    "                # print(f\"Loaded model from {model_filename,model_number}\")\n",
    "                train_ds, val_ds = dataprep(int(model_number))\n",
    "\n",
    "                # Calculate the confusion matrix\n",
    "                predictions = spock_model2.predict(val_ds)\n",
    "                predictedLabels = predictionLabels(predictions)\n",
    "                cm = confusion_matrix(val_df['class'].values, predictedLabels)\n",
    "\n",
    "                # Calculate the confusion matrix as percentages\n",
    "                cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "                # Create a ConfusionMatrixDisplay for the percentage confusion matrix\n",
    "                disp = ConfusionMatrixDisplay(confusion_matrix=cm_percentage, display_labels=['Toxic', 'Non-Toxic'])\n",
    "\n",
    "                # Calculate the classification report\n",
    "                clf_report = classification_report(val_df['class'],\n",
    "                                                   predictedLabels,\n",
    "                                                   target_names=['Toxic', 'Non-Toxic'],\n",
    "                                                   output_dict=True)\n",
    "\n",
    "\n",
    "                # Create a new figure for the combined plot\n",
    "                fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "                # Plot the confusion matrix as percentages on the left\n",
    "                disp.plot(cmap=plt.cm.Blues, values_format=\".2f\", ax=axs[0])\n",
    "                axs[0].set_title(f'Confusion Matrix of CS_{model_number}')\n",
    "\n",
    "                # Plot the classification report as a heatmap on the right\n",
    "                sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, ax=axs[1])\n",
    "                axs[1].set_title(f'Classification Report of CS_{model_number}')\n",
    "\n",
    "                # Adjust spacing between subplots\n",
    "                plt.subplots_adjust(wspace=0.5)\n",
    "                \n",
    "                \n",
    "                           # Save the combined plot as a single image\n",
    "                plt.savefig(f'{folder_path}/combined_CS_{model_number}.png')\n",
    "                plot(history,f'{folder_path}/Acc_loss{model_number}.png')\n",
    "\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Weights and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "U7gwzlktUtH9",
    "outputId": "56a7955b-b7bf-49f7-c6b5-135b4bcb0926"
   },
   "outputs": [],
   "source": [
    "# UNCOMMENT THIS CELL AFTER MODEL TRAINING\n",
    "\n",
    "# save model history\n",
    "\n",
    "with open(f\"{BASE_PATH}/training_historyV4.pkl\",\"wb\") as hist:\n",
    "  pickle.dump(history.history,hist)\n",
    "\n",
    "# history=np.save(f\"/home/naseem_fordham/Hate_Xplain/history/C_loss_history_{PROJECTION_DIM}.npy\",history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/naseem_fordham/Spock-paper/Spock_HateXplain\"\n",
    "model.load_weights(f\"{BASE_PATH}/test3.h5\")\n",
    "history=np.load(f\"{BASE_PATH}/training_historyV4.pkl\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23gCGD8ZeJfE",
    "outputId": "43e84bd0-3285-47e0-9b26-f5ed8ba6102f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # BASE_PATH='/home/naseem_fordham/Spock-paper/'\n",
    "# model.load_weights(f\"{BASE_PATH}/Modeltest1.h5\")\n",
    "# history=np.load(f\"{BASE_PATH}/training_historyV4.pkl\",allow_pickle=True)\n",
    "# history\n",
    "# import pickle\n",
    "\n",
    "# with open('/home/naseem_fordham/Spock-paper/Random_w/Model/training_historyV4.pkl', 'rb') as f:\n",
    "#     loaded_data = np.load(f,allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3kEUFa9eRfk"
   },
   "outputs": [],
   "source": [
    "# prepare test data for evaluation:\n",
    "test_gen   = dataset(test_df[\"tweet\"].values,test_df[\"class\"].values,max_length = MAX_LENGTH, tokenizer = tokenizer, projection_dim=PROJECTION_DIM, val = True)\n",
    "test_ds = tf.data.Dataset.from_generator(test_gen,\n",
    "                                            output_signature = \n",
    "                                           ({\"input_ids\" : tf.TensorSpec(shape = (MAX_LENGTH,), dtype = tf.int32 ),\"attention_masks\":tf.TensorSpec(shape = (MAX_LENGTH,), dtype = tf.int32),\"space\":tf.TensorSpec(shape = (PROJECTION_DIM,), dtype = tf.int32)},\n",
    "                                            tf.TensorSpec(shape = (), dtype = (tf.float32)))).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VcYiveLe-Ki"
   },
   "outputs": [],
   "source": [
    "# convert sigmoid outputs to labels\n",
    "def predictionLabels(i):\n",
    "     return np.argmax(i, axis=1)\n",
    "\n",
    "  # if i < 0.5:\n",
    "  #   return 0.0\n",
    "  # else:\n",
    "  #   return 1.0\n",
    "\n",
    "# getLabels = np.vectorize(predictionLabels)\n",
    "predictions = model.predict(test_ds)\n",
    "predictedLabels = predictionLabels(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictedLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-Cv6nLJsSsL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "predictedLabels = predictionLabels(predictions)\n",
    "\n",
    "confusion_matrix(test_df['class'].values, predictedLabels)\n",
    "ConfusionMatrixDisplay.from_predictions(test_df['class'].values, predictedLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(test_df['class'].values, predictedLabels)\n",
    "\n",
    "# Calculate the confusion matrix as percentages\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Create a ConfusionMatrixDisplay for the percentage confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percentage, display_labels=['0','1','2'])  # You should define class_labels\n",
    "\n",
    "# Plot the confusion matrix as percentages\n",
    "disp.plot(cmap=plt.cm.Blues, values_format=\".2f\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "nIV_XQ00gUYz",
    "outputId": "a28ee2c7-3d22-4fa8-a4fc-16b73454c70b"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# print(classification_report(y_test, predictedLabels))\n",
    "clf_report = classification_report(test_df['class'],\n",
    "                                   predictedLabels,\n",
    "                                   \n",
    "                                   target_names=[0,1,2],\n",
    "                                   output_dict=True)\n",
    "\n",
    "\n",
    "sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "-1xVwjAJgVrt",
    "outputId": "2b7f24a8-291e-4f8b-a5a3-cf9d89f07aaa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.title('model accuracy',fontdict = {'fontsize' : 12})\n",
    "plt.ylim(0,1,0.1)\n",
    "\n",
    "plt.ylabel('accuracy',fontdict = {'fontsize' : 12})\n",
    "plt.xlabel('epoch',{'fontsize' : 12})\n",
    "# plt.ylim(0, ,0.05)\n",
    "plt.legend(['train', 'val'], loc='upper left',fontsize=12)\n",
    "plt.title('Training vs Validation accuracy')\n",
    "# display(plt.show())\n",
    "# plt.show()\n",
    "\n",
    "# plt.savefig(f\"/home/naseem_fordham/Hate_Xplain//acc.png\",dpi=300)\n",
    "# plt.savefig(f\"/home/naseem_fordham/Hate_Xplain/Plots/plots{PROJECTION_DIM}/accu_{PROJECTION_DIM}.png\",dpi=300)\n",
    "\n",
    "#skip: plt.savefig(\"/gdrive/Shareddrives/Thesis/Results_for_thesis/spock_xhate_acc.png\",dpi=300)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "# plt.yticks(np.arange(0,1,step=.1))\n",
    "\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.ylim(0,3,0.1)\n",
    "\n",
    "# plt.title('Training loss vs Validation Loss',fontdict = {'fontsize' : 12})\n",
    "plt.ylabel('loss',fontdict = {'fontsize' : 12})\n",
    "plt.xlabel('epoch',fontdict = {'fontsize' : 12})\n",
    "plt.legend(['train', 'val'], loc='upper left',fontsize=12)\n",
    "plt.title('Training vs Validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lime Explainibity\n",
    "In this part we are using LIME method to understnd how our model is predicting each word in the senetcen and labeled it as per classification## In this part we are using LIME method to understnd how our model is predicting each word in the senetcen and labeled it as per classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for ele in train_ds.take(1):\n",
    "#   temp = ele\n",
    "# temp_iids = temp[0][\"input_ids\"]\n",
    "# # temp_mask = temp[0][\"attention_masks\"]\n",
    "# temp_iids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# laoding data set and performning cleaning to ready for feed funtion,\n",
    "# here we have assigned a tem class to our data set\n",
    "df_test=pd.read_csv('/home/naseem_fordham/Spock-paper/test.txt',sep='/n', header=None,engine='python')\n",
    "df_test = df_test.rename(columns={0: 'tweet'})\n",
    "df_test\n",
    "\n",
    "df_test[\"tweet\"] = df_test[\"tweet\"].apply(lambda x : text_preprocessing(x))\n",
    "df_test['class']=1\n",
    "df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create generators for train and validation\n",
    "BATCH_SIZE = 32\n",
    "# make sure batch size complies with total data set\n",
    "lime_gen = dataset(df_test[\"tweet\"].values,df_test[\"class\"].values,max_length = MAX_LENGTH, tokenizer = tokenizer,projection_dim=PROJECTION_DIM)\n",
    "\n",
    "# create tensorflow dataloaders from generators\n",
    "lime_ds = tf.data.Dataset.from_generator(lime_gen,\n",
    "                                            output_signature =\n",
    "                                           ( {\"input_ids\" : tf.TensorSpec(shape = (MAX_LENGTH,), dtype = tf.int32 ),\"attention_masks\":tf.TensorSpec(shape = (MAX_LENGTH,), dtype = tf.int32),\"space\":tf.TensorSpec(shape = (PROJECTION_DIM,), dtype = tf.int32)},\n",
    "                                            tf.TensorSpec(shape = (), dtype = (tf.float32)))).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict function which will be use later for each text in sentence\n",
    "def predict_fun(x):\n",
    "    return model.predict(lime_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.predict(lime_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for ele in lime_ds.take(1):\n",
    "#     t=ele\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# x=df_test['tweet'][0]\n",
    "# print(len(x))\n",
    "\n",
    "\n",
    "# explainer = LimeTextExplainer(class_names=['non_hate','hate'])\n",
    "# exp=explainer.explain_instance(x, predict_fun, num_features=90, labels=(1,), num_samples=9, distance_metric='cosine')\n",
    "# exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df_test['tweet'].iloc[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implementing LIME on each sentence\n",
    "\"\"\"Interpretability: If you want highly interpretable explanations that focus on the most salient \n",
    "words or terms, you may choose a lower num_features value.\n",
    "\n",
    "Comprehensiveness: If you want a more comprehensive understanding of why the model made a particular\n",
    "prediction and are willing to explore a larger number of words or terms, you may choose a higher num_features value.\"\"\"\n",
    "\n",
    "\n",
    "''' 0 - hate speech 1 - offensive language 2 - neither'''\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "for i in range(1,10):\n",
    "\n",
    "    x=df_test['tweet'].iloc[i]\n",
    "    # num=len(df_test['tweet'].iloc[i].split())\n",
    "    \n",
    "\n",
    "    explainer = LimeTextExplainer(class_names=['hate','offensive','normal'])\n",
    "    exp=explainer.explain_instance(x, predict_fun, num_features=6, labels=(0,1), num_samples=10, distance_metric='cosine')\n",
    "    exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lime EXplaniation Alternative method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for ele in lime_ds.take(0):\n",
    "#   temp = ele\n",
    "# temp_iids = temp[0]\n",
    "# # # temp_mask = temp[0][\"attention_masks\"]\n",
    "# temp_iids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_res= list()\n",
    "# for tweet in df_test['tweet']:\n",
    "#   tweet = text_preprocessing(tweet)\n",
    "#   test_res.append(tweet)\n",
    "#     # print(tweet)\n",
    "\n",
    "# df_test['tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96kydpg1iWwm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Input_ids=Inputs_test.reshape((Inputs_test.shape[0],1,Inputs_test.shape[1]))\n",
    "# # bertModel = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "# # tokenizer \n",
    "# '''In this part we are creating the bert inputs for our model and pass it to the model to predicts the class. \n",
    "# Later on we pass this predict model to LIME to underrstand which part of text is more relavent as per our model prediction'''\n",
    "\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# bmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# import torch\n",
    "# def predict(x):\n",
    "#     encoded = tokenizer(\n",
    "#     text=df_test['tweet'].tolist(),  # the sentence to be encoded\n",
    "#     add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "#     max_length = 45,  # maximum length of a sentence\n",
    "#     padding='max_length',  # Add [PAD]s\n",
    "#     return_attention_mask = True,  # Generate the attention mask\n",
    "#     return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "\n",
    "#   )\n",
    "#   # print(encoded)\n",
    "#     with torch.no_grad():\n",
    "\n",
    "#         outputs = bmodel(**encoded)\n",
    "\n",
    "#         # Evaluating the model will return a different number of objects based on \n",
    "#         # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "#         # becase we set `output_hidden_states = True`, the third item will be the \n",
    "#         # hidden states from all layers. See the documentation for more details:\n",
    "#         # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "#         # hidden_states = outputs[2]\n",
    "#         # violent_hidden_states = violent_outputs[2]\n",
    "\n",
    "#         last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "#     # print(last_hidden_states)\n",
    "\n",
    "#     x_test=last_hidden_states.numpy()\n",
    "#     # print(x_test.shape)\n",
    "#     Inputs_test=encoded['input_ids']\n",
    "#     # print(Inputs_test.shape)\n",
    "#     Inputs_test=Inputs_test.reshape((Inputs_test.shape[0],1,Inputs_test.shape[1])).numpy()\n",
    "#     print(Inputs_test.shape)\n",
    "\n",
    "\n",
    "#     # print(x_test.shape,Inputs_test.shape)\n",
    "#     embedding_test=embedding_index[0].reshape(embedding_index[0].shape[0],1)\n",
    "#     # embedding_test=embedding_index[:30]\n",
    "#   # embedding_test=embedding_index[:30].reshape(30,embedding_index[:30].shape[1],1)\n",
    "#   # embedding_test=embedding_index[:10].reshape(10,embedding_index.shape[1])\n",
    "#   # return model.predict([x_test,Inputs_test,embedding_test])\n",
    "  \n",
    "#     # print(embedding_test.shape)\n",
    "#     print(x_test.shape,Inputs_test.shape,embedding_test.shape)\n",
    "#     return np.array([[float(1-x), float (x)] for x in model.predict(lime_ds)])\n",
    "#     # return last_hidden_states\n",
    "# # model.predict([x_train,Input_ids,embedding_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def return_embedding_index(count):\n",
    "  \n",
    "#   embedding_index=np.array([i for i in range(count)])\n",
    "#   # embeding_index=np.array([[0,1,2]])\n",
    "#   embeding_index=np.ravel(embedding_index)\n",
    "\n",
    "#   embedding_index=np.tile(embedding_index,(len(df_test),1,))\n",
    "#   # print(embedding_index.shape, type(embeding_index))\n",
    "#   return embedding_index\n",
    "\n",
    "# embedding_index = return_embedding_index(PROJECTION_DIM)\n",
    "# embedding_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# x=df_test['tweet'][0]\n",
    "# print(len(x))\n",
    "\n",
    "\n",
    "# explainer = LimeTextExplainer(class_names=['peace','offensive'])\n",
    "# exp=explainer.explain_instance(x, predict, num_features=60, labels=(1,), num_samples=9, distance_metric='cosine')\n",
    "# #num of sample must be same as length of the data set \n",
    "# exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from lime.lime_text import LimeTextExplainer\n",
    "# for i in range(10):\n",
    "\n",
    "#     x=df_test['tweet'].iloc[i]\n",
    "\n",
    "#     explainer = LimeTextExplainer(class_names=['peace','offensive'])\n",
    "#     exp=explainer.explain_instance(x, predict, num_features=30, labels=(1,), num_samples=9, distance_metric='cosine')\n",
    "#     #num of sample must be same as length of the data set \n",
    "#     exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Concept Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# model.load_weights(f\"{BASE_PATH}/Modeltest1.h5\")\n",
    "\n",
    "\n",
    "hate_layer = model.get_layer('hate_embedding')\n",
    "hate_embedding = hate_layer.get_weights()\n",
    "# positive_weights=positive_weights[0].T\n",
    "\n",
    "\n",
    "offensive_layer = model.get_layer('offensive_embedding')\n",
    "offensive_embedding = offensive_layer.get_weights()\n",
    "\n",
    "\n",
    "normal_layer = model.get_layer('normal_embedding')\n",
    "normal_embedding = normal_layer.get_weights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# positive_embedding = model.get_layer('positive_embedding')  # Replace with the name of your layer\n",
    "# # Get the weights of the specific layer\n",
    "# positive_embedding = specific_layer.get_weights()\n",
    "# positive_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # model.load_weights(f\"{BASE_PATH}/Modeltest1.h5\")\n",
    "# negative_embedding = model.get_layer('negative_embedding')  # Replace with the name of your layer\n",
    "# # Get the weights of the specific layer\n",
    "# negative_embedding = specific_layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming you have two weight vectors of shape (10, 768)\n",
    "\n",
    "\n",
    "\n",
    "# # Combine the two weight vectors into one array\n",
    "# combined_weight_vectors = np.vstack([offensive_embedding[0], hate_embedding[0],normal_embedding[0]])\n",
    "# tsne = TSNE(n_components=2, perplexity=2, early_exaggeration=12.0, learning_rate=20.0, n_iter=1000)\n",
    "# # Compute t-SNE embeddings\n",
    "# # tsne = TSNE(n_components=2, random_state=42)\n",
    "# tsne_embeddings = tsne.fit_transform(combined_weight_vectors)\n",
    "\n",
    "# # Separate the t-SNE embeddings for the two weight vectors\n",
    "# tsne_embeddings1 = tsne_embeddings[:25]  # First weight vector\n",
    "# tsne_embeddings2 = tsne_embeddings[25:50]  # Second weight vector\n",
    "# tsne_embeddings3 = tsne_embeddings[50:] \n",
    "\n",
    "# # Create a scatter plot for the t-SNE embeddings\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(tsne_embeddings1[:, 0], tsne_embeddings1[:, 1], label='hate_embedding', s=5)\n",
    "# plt.scatter(tsne_embeddings2[:, 0], tsne_embeddings2[:, 1], label='offensive_embedding', s=5)\n",
    "# plt.scatter(tsne_embeddings3[:, 0], tsne_embeddings3[:, 1], label='offensive_embedding', s=5)\n",
    "# # plt.scatter(tsne_embeddings2[:, 0], tsne_embeddings2[:, 1], label='normal_embedding', s=5)\n",
    "\n",
    "# plt.xlabel('t-SNE Dimension 1')\n",
    "# plt.ylabel('t-SNE Dimension 2')\n",
    "# plt.legend()\n",
    "# plt.title('t-SNE Visualization of Weight Vectors')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import the 3D plotting module\n",
    "\n",
    "# Assuming you have three weight vectors of shape (10, 768)\n",
    "\n",
    "# Combine the three weight vectors into one array\n",
    "combined_weight_vectors = np.vstack([hate_embedding[0], offensive_embedding[0], normal_embedding[0]])\n",
    "tsne = TSNE(n_components=3, perplexity=50, early_exaggeration=12.0, learning_rate=50.0, n_iter=10000)\n",
    "\n",
    "# Compute t-SNE embeddings\n",
    "tsne_embeddings = tsne.fit_transform(combined_weight_vectors)\n",
    "\n",
    "# Separate the t-SNE embeddings for the three weight vectors\n",
    "tsne_embeddings1 = tsne_embeddings[:25]        # First weight vector (hate)\n",
    "tsne_embeddings2 = tsne_embeddings[25:50]      # Second weight vector (offensive)\n",
    "tsne_embeddings3 = tsne_embeddings[50:]        # Third weight vector (normal)\n",
    "\n",
    "# Create a 3D scatter plot for the t-SNE embeddings\n",
    "fig = plt.figure(figsize=(8, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')  # Create a 3D axis\n",
    "\n",
    "ax.scatter(tsne_embeddings1[:, 0], tsne_embeddings1[:, 1], tsne_embeddings1[:, 2], label='hate_embedding', s=5)\n",
    "ax.scatter(tsne_embeddings2[:, 0], tsne_embeddings2[:, 1], tsne_embeddings2[:, 2], label='offensive_embedding', s=5)\n",
    "ax.scatter(tsne_embeddings3[:, 0], tsne_embeddings3[:, 1], tsne_embeddings3[:, 2], label='normal_embedding', s=5)\n",
    "\n",
    "ax.set_xlabel('t-SNE Dimension 1')\n",
    "ax.set_ylabel('t-SNE Dimension 2')\n",
    "ax.set_zlabel('t-SNE Dimension 3')\n",
    "plt.legend()\n",
    "plt.title('3D t-SNE Visualization of Weight Vectors')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyMciCgoRuYPP4D87bGoAjIQ",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06285d669e92494192637cb3ee5a40f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f14c359d92748268810099e6cdd1fc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06285d669e92494192637cb3ee5a40f4",
      "placeholder": "​",
      "style": "IPY_MODEL_209ee3e0949b446b8c58c57989065c5a",
      "value": " 232k/232k [00:00&lt;00:00, 2.84MB/s]"
     }
    },
    "123641d1426541fba838576dff7f6082": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "17ae300ebf634778862bc211cb432eef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a51f1ae4e3bd4563be3eb7c04dec7a60",
      "placeholder": "​",
      "style": "IPY_MODEL_c7586ac9b41c4e75b02a6c076a458686",
      "value": "Downloading (…)/main/tokenizer.json: 100%"
     }
    },
    "18e134c7af5d45baa952ccf06f96f3d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f1d5a79d7ec4e42add7775fa925b187",
      "placeholder": "​",
      "style": "IPY_MODEL_e89d668a4fb541ec8d75d171bf899869",
      "value": "Downloading (…)solve/main/vocab.txt: 100%"
     }
    },
    "1cf66f2a8c5f49ce98d59dae5186298d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dfd44792f624499b87bd768836016e88",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae841edad05b4ef9b4308308d6683fa3",
      "value": 570
     }
    },
    "1ec35f8bfadb40968f777ab7bbb55184": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b0c918326174bc4b80585a52ac33e32",
      "placeholder": "​",
      "style": "IPY_MODEL_e38ea829111b4065bad8ad4a4927bc39",
      "value": " 466k/466k [00:00&lt;00:00, 8.67MB/s]"
     }
    },
    "2024092105904b418c984aae9f4118bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f0a4e1b4929448a8b0a25b202271a40",
      "placeholder": "​",
      "style": "IPY_MODEL_ac93c61a19c0474994803853194d2aa8",
      "value": " 28.0/28.0 [00:00&lt;00:00, 584B/s]"
     }
    },
    "209ee3e0949b446b8c58c57989065c5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "21bf2e03eac8419fa693628ab2cef02d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_430a9d1768614e0596960d2a3d15d69e",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_909ae03e117345f7a4b53f3045e090b7",
      "value": 231508
     }
    },
    "274875a4aa6047c9903ae3065a5d85c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31914a14cfa243388f43a02da4db24b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3258d106659040dfb7ffea47c017ed38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34bbd1edf06c4a9194bd27eaeeee32b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34e574772e8b4f81bc7e02e8ebaeb258": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3258d106659040dfb7ffea47c017ed38",
      "placeholder": "​",
      "style": "IPY_MODEL_f9a2caba063b4a4f98ad258044d012fb",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "3c294ab0d9a648f39ea3f3fb7dd08c3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "430a9d1768614e0596960d2a3d15d69e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4914d10c0f354e178de66dc11449e26a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7998e50af804d66b6af0f26b5588255",
      "placeholder": "​",
      "style": "IPY_MODEL_4fa52eecbe8a4ba38ef969fd1cf48cfd",
      "value": "Downloading model.safetensors: 100%"
     }
    },
    "4cc5655f9bdb42bca5d80e2849e8cd2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f0a4e1b4929448a8b0a25b202271a40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f1d5a79d7ec4e42add7775fa925b187": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4fa52eecbe8a4ba38ef969fd1cf48cfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "50859aa086814457bcab249b35d486a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "570b9a872e614e0eab6618f306be7306": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_34e574772e8b4f81bc7e02e8ebaeb258",
       "IPY_MODEL_1cf66f2a8c5f49ce98d59dae5186298d",
       "IPY_MODEL_71f100b04c964a1bbde06c537f483ed7"
      ],
      "layout": "IPY_MODEL_9e1caf8b23fb4eb1976f808269d1dd3e"
     }
    },
    "5a072e1f73624ea5aa35e7e18af17f50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a0c04349bae44f4a7130463ded826d1",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9f11959bab964aad822250b6e6853cb8",
      "value": 466062
     }
    },
    "672e978f9b524c5784553abd8cc91507": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_17ae300ebf634778862bc211cb432eef",
       "IPY_MODEL_5a072e1f73624ea5aa35e7e18af17f50",
       "IPY_MODEL_1ec35f8bfadb40968f777ab7bbb55184"
      ],
      "layout": "IPY_MODEL_274875a4aa6047c9903ae3065a5d85c2"
     }
    },
    "69d90bdca5174dfb91f5cf124884095e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71d0c2b02f0c4a609477bc31aa880938": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c12c857f159443f1b2a661fcd7afa002",
       "IPY_MODEL_989933f9c84a47d4b64d5b5441a1f492",
       "IPY_MODEL_2024092105904b418c984aae9f4118bf"
      ],
      "layout": "IPY_MODEL_4cc5655f9bdb42bca5d80e2849e8cd2b"
     }
    },
    "71f100b04c964a1bbde06c537f483ed7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34bbd1edf06c4a9194bd27eaeeee32b2",
      "placeholder": "​",
      "style": "IPY_MODEL_ea850e74cb6a4121a456578b29ce7955",
      "value": " 570/570 [00:00&lt;00:00, 5.04kB/s]"
     }
    },
    "8a0c04349bae44f4a7130463ded826d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "909ae03e117345f7a4b53f3045e090b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "989933f9c84a47d4b64d5b5441a1f492": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9910be9c8b7b4188936721e958ca15c5",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_50859aa086814457bcab249b35d486a8",
      "value": 28
     }
    },
    "9910be9c8b7b4188936721e958ca15c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b0c918326174bc4b80585a52ac33e32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e1caf8b23fb4eb1976f808269d1dd3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f11959bab964aad822250b6e6853cb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a51f1ae4e3bd4563be3eb7c04dec7a60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac93c61a19c0474994803853194d2aa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae841edad05b4ef9b4308308d6683fa3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b5516712d5304670a05ec871eb5896a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c12c857f159443f1b2a661fcd7afa002": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd56d3ae17774e5496a7fdcbcb57b874",
      "placeholder": "​",
      "style": "IPY_MODEL_b5516712d5304670a05ec871eb5896a4",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "c7586ac9b41c4e75b02a6c076a458686": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cfe933364a3147368660f5463f1e5a29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d69dfd21bce04204b3220b71b4d4a968": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4914d10c0f354e178de66dc11449e26a",
       "IPY_MODEL_f554ec9fe80a408680aa7b0a1c6837ac",
       "IPY_MODEL_f4ec333b7c654876a6caad010e88c203"
      ],
      "layout": "IPY_MODEL_cfe933364a3147368660f5463f1e5a29"
     }
    },
    "dfd44792f624499b87bd768836016e88": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e38ea829111b4065bad8ad4a4927bc39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e89d668a4fb541ec8d75d171bf899869": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e99b3d05437f4cdf9e1700e3cf466b34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea850e74cb6a4121a456578b29ce7955": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee12049f5a9c4505b9769a4ff9c36477": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_18e134c7af5d45baa952ccf06f96f3d8",
       "IPY_MODEL_21bf2e03eac8419fa693628ab2cef02d",
       "IPY_MODEL_0f14c359d92748268810099e6cdd1fc3"
      ],
      "layout": "IPY_MODEL_e99b3d05437f4cdf9e1700e3cf466b34"
     }
    },
    "f4ec333b7c654876a6caad010e88c203": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69d90bdca5174dfb91f5cf124884095e",
      "placeholder": "​",
      "style": "IPY_MODEL_123641d1426541fba838576dff7f6082",
      "value": " 440M/440M [00:06&lt;00:00, 121MB/s]"
     }
    },
    "f554ec9fe80a408680aa7b0a1c6837ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c294ab0d9a648f39ea3f3fb7dd08c3c",
      "max": 440449768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_31914a14cfa243388f43a02da4db24b6",
      "value": 440449768
     }
    },
    "f7998e50af804d66b6af0f26b5588255": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9a2caba063b4a4f98ad258044d012fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd56d3ae17774e5496a7fdcbcb57b874": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
